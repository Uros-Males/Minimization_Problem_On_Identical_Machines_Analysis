{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEhh5qgetVAjVKhKOA1owt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uros-Males/Minimization_Problem_On_Identical_Machines_Analysis/blob/main/NEWNEW_MIXED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "amvyGSys0OkP"
      },
      "outputs": [],
      "source": [
        "#IN PROGRESS....\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "import time\n",
        "import keras\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/sample_data/C-instances-runtime-analysis.csv')\n",
        "\n",
        "\n",
        "\n",
        "df = df[df['n/m']==2]\n",
        "\n",
        "shuffled = df.sample(frac=1).reset_index()\n",
        "\n",
        "columns_dont_want = [ 'index', 'inst.name','type', 'CPLEXStatus'] #cple\n",
        "select = [x for x in shuffled.columns if x not in columns_dont_want]\n",
        "\n",
        "X = shuffled.loc[:, select]\n",
        "\n",
        "y = shuffled.loc[:,'y']\n",
        "names = ['indeks', 'n', 'n/m', '(n/m)^2', '(n/m)^3', 'm/n', 'av.length', 'std.dev', 'median', 'range', 'min', 'max', 'k']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_new = []\n",
        "cnt0=0\n",
        "cnt1=0\n",
        "cnt2=0\n",
        "cnt3=0\n",
        "for val in y:\n",
        "  if(val<10):\n",
        "    y_new.append(0)\n",
        "    cnt0+=1\n",
        "  elif(10 <= val and val<1000):\n",
        "    y_new.append(1)\n",
        "    cnt1+=1\n",
        "  else:\n",
        "    y_new.append(2)\n",
        "    cnt2+=1\n",
        "\n",
        "cnt = 0\n",
        "if(cnt0>0):\n",
        "  cnt+=1\n",
        "if(cnt1>0):\n",
        "  cnt+=1\n",
        "if(cnt2>0):\n",
        "  cnt+=1\n",
        "if(cnt3>0):\n",
        "  cnt+=1\n",
        "\n",
        "shuffled['y_new'] = y_new\n",
        "y_encoded = shuffled.loc[:,'y_new']\n",
        "y_new = np.array(y_new)"
      ],
      "metadata": {
        "id": "1jk4i-n54rQB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names = [ 'av.length', 'n', 'std.dev', 'k', 'm', 'max']\n",
        "X_modified = X.drop([   'n/m', 'indeks', 'class',  'subtype', '(m/n)^3', '(m/n)^2', '(n/m)^2', '(n/m)^3','m/n','median', 'min', 'range'], axis = 1)\n",
        "print(X_modified.head())\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "oversample = RandomOverSampler(random_state=0)\n",
        "X_modified, y_encoded = oversample.fit_resample(X_modified, y_encoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKaCxrM840wN",
        "outputId": "9796cbda-554a-442d-96df-cbb267da50a1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     n    m   av.length     std.dev  max    k          y\n",
            "0  180   90   74.783333   14.444373  100   51  15.495687\n",
            "1  120   60   47.241665   27.169245   99   70   1.217628\n",
            "2  180   90  455.455566  161.580276  719  150   6.639741\n",
            "3  140   70  574.464294  134.110840  894  117   3.155640\n",
            "4  200  100   63.680000   24.820713  100   75  28.157019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "l_encode = LabelEncoder()\n",
        "l_encode.fit(y_encoded)\n",
        "y_encoded = l_encode.transform(y_encoded)\n",
        "y_encoded = to_categorical(y_encoded)\n",
        "y_encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxf3SgQ_41yz",
        "outputId": "57eb011b-543c-4a20-cee3-fcb6a9984911"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc = StandardScaler()\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_modified, y_encoded, random_state=0, train_size = 0.83)\n",
        "print(X_train.shape[0])\n",
        "print(X_test.shape[0])\n",
        "\n",
        "X_train = X_train.drop(['y', 'k', 'm'], axis = 1)\n",
        "\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test_copy = X_test\n",
        "print(X_test_copy.head())\n",
        "X_test = X_test.drop(['y', 'k', 'm'], axis = 1)\n",
        "X_test = sc.transform(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0B7m6o_X45DF",
        "outputId": "895b9a2c-1a00-4074-99ab-e65de617e9c4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1267\n",
            "260\n",
            "        n   m   av.length     std.dev   max    k             y\n",
            "1362  180  90  706.877808  175.294418  1104  154  99999.000000\n",
            "511    80  40   52.587502   26.850121   100   56     11.129904\n",
            "9     120  60  301.091675  102.802505   480  107      3.469282\n",
            "393    40  20  102.425003   22.303143   160   31      0.654424\n",
            "471   140  70  569.014282  133.063202   914  127      2.286228\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "\n",
        "classifier = Sequential()\n",
        "classifier.add(Dense(8, input_dim = X_modified.shape[1]-3, activation='relu'))\n",
        "\n",
        "classifier.add(Dense(12, activation = 'relu'))\n",
        "classifier.add(Dropout(0.125))\n",
        "\n",
        "#classifier.add(Dense(32, activation = 'relu'))\n",
        "\n",
        "#classifier.add(Dense(32, activation = 'relu'))\n",
        "#classifier.add(Dense(32, activation = 'relu'))\n",
        "#classifier.add(Dense(16, activation = 'relu'))\n",
        "\n",
        "#classifier.add(Dense(16, activation='relu'))\n",
        "\n",
        "#classifier.add(Dense(8, activation='relu'))\n",
        "\n",
        "#classifier.add(Dense(12, activation='relu'))\n",
        "\n",
        "classifier.add(Dense(cnt, activation = 'softmax'))\n",
        "classifier.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "classifier.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHzkCr_P48Iw",
        "outputId": "eca739d9-8247-4bee-b405-224c722da0ca"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 8)                 40        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 12)                108       \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 12)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 3)                 39        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 187\n",
            "Trainable params: 187\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.metrics import accuracy\n",
        "from keras import callbacks\n",
        "earlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", \n",
        "                                        mode =\"min\", patience = 25, \n",
        "                                        restore_best_weights = True)\n",
        "  \n",
        "history = classifier.fit(X_train, y_train, batch_size = 64, \n",
        "                    epochs = 350, shuffle = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HxWFp7_5AoP",
        "outputId": "ab2a2fcc-f922-4fc4-c075-7628f53bd9b2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/350\n",
            "20/20 [==============================] - 1s 5ms/step - loss: 1.1259 - accuracy: 0.3386\n",
            "Epoch 2/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 1.0561 - accuracy: 0.3662\n",
            "Epoch 3/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.9976 - accuracy: 0.4507\n",
            "Epoch 4/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.9542 - accuracy: 0.5343\n",
            "Epoch 5/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.9118 - accuracy: 0.5746\n",
            "Epoch 6/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.8778 - accuracy: 0.5975\n",
            "Epoch 7/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.8392 - accuracy: 0.6172\n",
            "Epoch 8/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.8083 - accuracy: 0.6409\n",
            "Epoch 9/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.7749 - accuracy: 0.6488\n",
            "Epoch 10/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.7359 - accuracy: 0.6875\n",
            "Epoch 11/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.7133 - accuracy: 0.7269\n",
            "Epoch 12/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.6862 - accuracy: 0.7593\n",
            "Epoch 13/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.6810 - accuracy: 0.7601\n",
            "Epoch 14/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.6527 - accuracy: 0.7719\n",
            "Epoch 15/350\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.6346 - accuracy: 0.7640\n",
            "Epoch 16/350\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.6220 - accuracy: 0.7695\n",
            "Epoch 17/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.6001 - accuracy: 0.7837\n",
            "Epoch 18/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.5874 - accuracy: 0.7711\n",
            "Epoch 19/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.5783 - accuracy: 0.7782\n",
            "Epoch 20/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.5640 - accuracy: 0.7869\n",
            "Epoch 21/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.5548 - accuracy: 0.7908\n",
            "Epoch 22/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.5425 - accuracy: 0.7987\n",
            "Epoch 23/350\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.5348 - accuracy: 0.8011\n",
            "Epoch 24/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.5177 - accuracy: 0.8035\n",
            "Epoch 25/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.5220 - accuracy: 0.8019\n",
            "Epoch 26/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.5122 - accuracy: 0.8011\n",
            "Epoch 27/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.4993 - accuracy: 0.8106\n",
            "Epoch 28/350\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.4888 - accuracy: 0.8240\n",
            "Epoch 29/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.4740 - accuracy: 0.8264\n",
            "Epoch 30/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4785 - accuracy: 0.8350\n",
            "Epoch 31/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4635 - accuracy: 0.8350\n",
            "Epoch 32/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.4591 - accuracy: 0.8319\n",
            "Epoch 33/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4564 - accuracy: 0.8398\n",
            "Epoch 34/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4504 - accuracy: 0.8303\n",
            "Epoch 35/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4472 - accuracy: 0.8382\n",
            "Epoch 36/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4396 - accuracy: 0.8398\n",
            "Epoch 37/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4367 - accuracy: 0.8469\n",
            "Epoch 38/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4386 - accuracy: 0.8343\n",
            "Epoch 39/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4342 - accuracy: 0.8335\n",
            "Epoch 40/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4313 - accuracy: 0.8366\n",
            "Epoch 41/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4231 - accuracy: 0.8421\n",
            "Epoch 42/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4092 - accuracy: 0.8445\n",
            "Epoch 43/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4136 - accuracy: 0.8398\n",
            "Epoch 44/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4193 - accuracy: 0.8469\n",
            "Epoch 45/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.4074 - accuracy: 0.8500\n",
            "Epoch 46/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4001 - accuracy: 0.8508\n",
            "Epoch 47/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.3999 - accuracy: 0.8500\n",
            "Epoch 48/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4051 - accuracy: 0.8390\n",
            "Epoch 49/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.3977 - accuracy: 0.8500\n",
            "Epoch 50/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.3814 - accuracy: 0.8556\n",
            "Epoch 51/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.3997 - accuracy: 0.8429\n",
            "Epoch 52/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.3741 - accuracy: 0.8508\n",
            "Epoch 53/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.3919 - accuracy: 0.8540\n",
            "Epoch 54/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.3802 - accuracy: 0.8595\n",
            "Epoch 55/350\n",
            "20/20 [==============================] - 0s 19ms/step - loss: 0.3750 - accuracy: 0.8540\n",
            "Epoch 56/350\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.3723 - accuracy: 0.8627\n",
            "Epoch 57/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.3768 - accuracy: 0.8532\n",
            "Epoch 58/350\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.3641 - accuracy: 0.8674\n",
            "Epoch 59/350\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.3761 - accuracy: 0.8564\n",
            "Epoch 60/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.3599 - accuracy: 0.8595\n",
            "Epoch 61/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.3667 - accuracy: 0.8666\n",
            "Epoch 62/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.3680 - accuracy: 0.8587\n",
            "Epoch 63/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.3505 - accuracy: 0.8611\n",
            "Epoch 64/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.3604 - accuracy: 0.8595\n",
            "Epoch 65/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.3524 - accuracy: 0.8619\n",
            "Epoch 66/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.3593 - accuracy: 0.8571\n",
            "Epoch 67/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.3634 - accuracy: 0.8508\n",
            "Epoch 68/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.3466 - accuracy: 0.8682\n",
            "Epoch 69/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.3422 - accuracy: 0.8658\n",
            "Epoch 70/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.3515 - accuracy: 0.8587\n",
            "Epoch 71/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.3404 - accuracy: 0.8587\n",
            "Epoch 72/350\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.3419 - accuracy: 0.8682\n",
            "Epoch 73/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.3379 - accuracy: 0.8666\n",
            "Epoch 74/350\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.3390 - accuracy: 0.8698\n",
            "Epoch 75/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.3368 - accuracy: 0.8745\n",
            "Epoch 76/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.3319 - accuracy: 0.8737\n",
            "Epoch 77/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.3385 - accuracy: 0.8666\n",
            "Epoch 78/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.3323 - accuracy: 0.8729\n",
            "Epoch 79/350\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.3265 - accuracy: 0.8769\n",
            "Epoch 80/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.3325 - accuracy: 0.8690\n",
            "Epoch 81/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.3327 - accuracy: 0.8777\n",
            "Epoch 82/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.3297 - accuracy: 0.8785\n",
            "Epoch 83/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.3242 - accuracy: 0.8792\n",
            "Epoch 84/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.3278 - accuracy: 0.8761\n",
            "Epoch 85/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.3184 - accuracy: 0.8816\n",
            "Epoch 86/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.3132 - accuracy: 0.8840\n",
            "Epoch 87/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.3227 - accuracy: 0.8745\n",
            "Epoch 88/350\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.3143 - accuracy: 0.8800\n",
            "Epoch 89/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.3151 - accuracy: 0.8777\n",
            "Epoch 90/350\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.3092 - accuracy: 0.8832\n",
            "Epoch 91/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.3232 - accuracy: 0.8785\n",
            "Epoch 92/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.3157 - accuracy: 0.8777\n",
            "Epoch 93/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.3176 - accuracy: 0.8792\n",
            "Epoch 94/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.3183 - accuracy: 0.8800\n",
            "Epoch 95/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.3095 - accuracy: 0.8785\n",
            "Epoch 96/350\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.3130 - accuracy: 0.8792\n",
            "Epoch 97/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.3056 - accuracy: 0.8927\n",
            "Epoch 98/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.3017 - accuracy: 0.8950\n",
            "Epoch 99/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2980 - accuracy: 0.8800\n",
            "Epoch 100/350\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.3150 - accuracy: 0.8848\n",
            "Epoch 101/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.3057 - accuracy: 0.8848\n",
            "Epoch 102/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.3060 - accuracy: 0.8863\n",
            "Epoch 103/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.3036 - accuracy: 0.8903\n",
            "Epoch 104/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2982 - accuracy: 0.8934\n",
            "Epoch 105/350\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.2889 - accuracy: 0.8966\n",
            "Epoch 106/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2915 - accuracy: 0.8966\n",
            "Epoch 107/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.2875 - accuracy: 0.9006\n",
            "Epoch 108/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2830 - accuracy: 0.8966\n",
            "Epoch 109/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2859 - accuracy: 0.8919\n",
            "Epoch 110/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2924 - accuracy: 0.8927\n",
            "Epoch 111/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2899 - accuracy: 0.8982\n",
            "Epoch 112/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2829 - accuracy: 0.8942\n",
            "Epoch 113/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2819 - accuracy: 0.8966\n",
            "Epoch 114/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2908 - accuracy: 0.8919\n",
            "Epoch 115/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2940 - accuracy: 0.8942\n",
            "Epoch 116/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2871 - accuracy: 0.8958\n",
            "Epoch 117/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2877 - accuracy: 0.8919\n",
            "Epoch 118/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2807 - accuracy: 0.9006\n",
            "Epoch 119/350\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.2813 - accuracy: 0.8974\n",
            "Epoch 120/350\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.2756 - accuracy: 0.8990\n",
            "Epoch 121/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2747 - accuracy: 0.8974\n",
            "Epoch 122/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2722 - accuracy: 0.9021\n",
            "Epoch 123/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2781 - accuracy: 0.8934\n",
            "Epoch 124/350\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.2722 - accuracy: 0.9116\n",
            "Epoch 125/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2815 - accuracy: 0.9037\n",
            "Epoch 126/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2734 - accuracy: 0.9021\n",
            "Epoch 127/350\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.2732 - accuracy: 0.9013\n",
            "Epoch 128/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2658 - accuracy: 0.9069\n",
            "Epoch 129/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2811 - accuracy: 0.9006\n",
            "Epoch 130/350\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.2714 - accuracy: 0.9053\n",
            "Epoch 131/350\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.2728 - accuracy: 0.8998\n",
            "Epoch 132/350\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.2692 - accuracy: 0.9037\n",
            "Epoch 133/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2735 - accuracy: 0.9037\n",
            "Epoch 134/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2712 - accuracy: 0.9116\n",
            "Epoch 135/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2673 - accuracy: 0.9069\n",
            "Epoch 136/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2699 - accuracy: 0.9029\n",
            "Epoch 137/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2662 - accuracy: 0.9077\n",
            "Epoch 138/350\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.2722 - accuracy: 0.9037\n",
            "Epoch 139/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2672 - accuracy: 0.9069\n",
            "Epoch 140/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2699 - accuracy: 0.9061\n",
            "Epoch 141/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2615 - accuracy: 0.9116\n",
            "Epoch 142/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2646 - accuracy: 0.9100\n",
            "Epoch 143/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.2543 - accuracy: 0.9092\n",
            "Epoch 144/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2586 - accuracy: 0.9124\n",
            "Epoch 145/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2567 - accuracy: 0.9116\n",
            "Epoch 146/350\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.2580 - accuracy: 0.9069\n",
            "Epoch 147/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2674 - accuracy: 0.9037\n",
            "Epoch 148/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2473 - accuracy: 0.9148\n",
            "Epoch 149/350\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.2632 - accuracy: 0.9100\n",
            "Epoch 150/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2651 - accuracy: 0.9045\n",
            "Epoch 151/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2595 - accuracy: 0.9037\n",
            "Epoch 152/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2463 - accuracy: 0.9108\n",
            "Epoch 153/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2566 - accuracy: 0.9100\n",
            "Epoch 154/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2612 - accuracy: 0.9021\n",
            "Epoch 155/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2514 - accuracy: 0.9108\n",
            "Epoch 156/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2602 - accuracy: 0.9053\n",
            "Epoch 157/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2544 - accuracy: 0.9045\n",
            "Epoch 158/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2577 - accuracy: 0.9092\n",
            "Epoch 159/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2597 - accuracy: 0.9029\n",
            "Epoch 160/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2544 - accuracy: 0.9053\n",
            "Epoch 161/350\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.2500 - accuracy: 0.9084\n",
            "Epoch 162/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2445 - accuracy: 0.9163\n",
            "Epoch 163/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2433 - accuracy: 0.9171\n",
            "Epoch 164/350\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.2428 - accuracy: 0.9124\n",
            "Epoch 165/350\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.2492 - accuracy: 0.9100\n",
            "Epoch 166/350\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.2499 - accuracy: 0.9195\n",
            "Epoch 167/350\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.2505 - accuracy: 0.9140\n",
            "Epoch 168/350\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.2462 - accuracy: 0.9211\n",
            "Epoch 169/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2508 - accuracy: 0.9124\n",
            "Epoch 170/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2416 - accuracy: 0.9179\n",
            "Epoch 171/350\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.2462 - accuracy: 0.9171\n",
            "Epoch 172/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2426 - accuracy: 0.9092\n",
            "Epoch 173/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2456 - accuracy: 0.9132\n",
            "Epoch 174/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2530 - accuracy: 0.9140\n",
            "Epoch 175/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2407 - accuracy: 0.9140\n",
            "Epoch 176/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2512 - accuracy: 0.9140\n",
            "Epoch 177/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2469 - accuracy: 0.9084\n",
            "Epoch 178/350\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.2320 - accuracy: 0.9219\n",
            "Epoch 179/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2445 - accuracy: 0.9171\n",
            "Epoch 180/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2414 - accuracy: 0.9108\n",
            "Epoch 181/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2373 - accuracy: 0.9219\n",
            "Epoch 182/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.2399 - accuracy: 0.9219\n",
            "Epoch 183/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2430 - accuracy: 0.9108\n",
            "Epoch 184/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2378 - accuracy: 0.9187\n",
            "Epoch 185/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2407 - accuracy: 0.9187\n",
            "Epoch 186/350\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.2310 - accuracy: 0.9171\n",
            "Epoch 187/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2408 - accuracy: 0.9148\n",
            "Epoch 188/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2309 - accuracy: 0.9148\n",
            "Epoch 189/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2425 - accuracy: 0.9084\n",
            "Epoch 190/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2345 - accuracy: 0.9187\n",
            "Epoch 191/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2264 - accuracy: 0.9203\n",
            "Epoch 192/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2317 - accuracy: 0.9155\n",
            "Epoch 193/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2360 - accuracy: 0.9140\n",
            "Epoch 194/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2345 - accuracy: 0.9140\n",
            "Epoch 195/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2364 - accuracy: 0.9227\n",
            "Epoch 196/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2316 - accuracy: 0.9140\n",
            "Epoch 197/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2316 - accuracy: 0.9211\n",
            "Epoch 198/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2271 - accuracy: 0.9227\n",
            "Epoch 199/350\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.2256 - accuracy: 0.9203\n",
            "Epoch 200/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2250 - accuracy: 0.9187\n",
            "Epoch 201/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2268 - accuracy: 0.9148\n",
            "Epoch 202/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2275 - accuracy: 0.9171\n",
            "Epoch 203/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2308 - accuracy: 0.9195\n",
            "Epoch 204/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2240 - accuracy: 0.9187\n",
            "Epoch 205/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2321 - accuracy: 0.9124\n",
            "Epoch 206/350\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.2281 - accuracy: 0.9187\n",
            "Epoch 207/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2262 - accuracy: 0.9242\n",
            "Epoch 208/350\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.2298 - accuracy: 0.9163\n",
            "Epoch 209/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2296 - accuracy: 0.9140\n",
            "Epoch 210/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2150 - accuracy: 0.9227\n",
            "Epoch 211/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2308 - accuracy: 0.9211\n",
            "Epoch 212/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2171 - accuracy: 0.9337\n",
            "Epoch 213/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2316 - accuracy: 0.9171\n",
            "Epoch 214/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2229 - accuracy: 0.9203\n",
            "Epoch 215/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2219 - accuracy: 0.9242\n",
            "Epoch 216/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2190 - accuracy: 0.9187\n",
            "Epoch 217/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2263 - accuracy: 0.9219\n",
            "Epoch 218/350\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.2250 - accuracy: 0.9266\n",
            "Epoch 219/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2091 - accuracy: 0.9313\n",
            "Epoch 220/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2221 - accuracy: 0.9282\n",
            "Epoch 221/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.2167 - accuracy: 0.9266\n",
            "Epoch 222/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2193 - accuracy: 0.9242\n",
            "Epoch 223/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2262 - accuracy: 0.9195\n",
            "Epoch 224/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2161 - accuracy: 0.9258\n",
            "Epoch 225/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2140 - accuracy: 0.9329\n",
            "Epoch 226/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2260 - accuracy: 0.9227\n",
            "Epoch 227/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2151 - accuracy: 0.9250\n",
            "Epoch 228/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2201 - accuracy: 0.9234\n",
            "Epoch 229/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2093 - accuracy: 0.9274\n",
            "Epoch 230/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.2233 - accuracy: 0.9219\n",
            "Epoch 231/350\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.2158 - accuracy: 0.9203\n",
            "Epoch 232/350\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.2168 - accuracy: 0.9211\n",
            "Epoch 233/350\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.2162 - accuracy: 0.9227\n",
            "Epoch 234/350\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.2066 - accuracy: 0.9313\n",
            "Epoch 235/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2079 - accuracy: 0.9266\n",
            "Epoch 236/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2136 - accuracy: 0.9250\n",
            "Epoch 237/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2076 - accuracy: 0.9266\n",
            "Epoch 238/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2196 - accuracy: 0.9274\n",
            "Epoch 239/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2177 - accuracy: 0.9305\n",
            "Epoch 240/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2116 - accuracy: 0.9258\n",
            "Epoch 241/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2197 - accuracy: 0.9227\n",
            "Epoch 242/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2113 - accuracy: 0.9298\n",
            "Epoch 243/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2181 - accuracy: 0.9211\n",
            "Epoch 244/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2038 - accuracy: 0.9353\n",
            "Epoch 245/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2154 - accuracy: 0.9227\n",
            "Epoch 246/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2122 - accuracy: 0.9282\n",
            "Epoch 247/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2198 - accuracy: 0.9250\n",
            "Epoch 248/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2061 - accuracy: 0.9298\n",
            "Epoch 249/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2074 - accuracy: 0.9266\n",
            "Epoch 250/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2194 - accuracy: 0.9171\n",
            "Epoch 251/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2163 - accuracy: 0.9266\n",
            "Epoch 252/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2107 - accuracy: 0.9242\n",
            "Epoch 253/350\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.2004 - accuracy: 0.9345\n",
            "Epoch 254/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2056 - accuracy: 0.9321\n",
            "Epoch 255/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2091 - accuracy: 0.9361\n",
            "Epoch 256/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.1997 - accuracy: 0.9329\n",
            "Epoch 257/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2084 - accuracy: 0.9305\n",
            "Epoch 258/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.1997 - accuracy: 0.9305\n",
            "Epoch 259/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2035 - accuracy: 0.9321\n",
            "Epoch 260/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.1967 - accuracy: 0.9345\n",
            "Epoch 261/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2074 - accuracy: 0.9282\n",
            "Epoch 262/350\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.2046 - accuracy: 0.9298\n",
            "Epoch 263/350\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.2105 - accuracy: 0.9258\n",
            "Epoch 264/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2116 - accuracy: 0.9274\n",
            "Epoch 265/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2032 - accuracy: 0.9274\n",
            "Epoch 266/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2073 - accuracy: 0.9298\n",
            "Epoch 267/350\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1985 - accuracy: 0.9353\n",
            "Epoch 268/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2019 - accuracy: 0.9313\n",
            "Epoch 269/350\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.2087 - accuracy: 0.9274\n",
            "Epoch 270/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2007 - accuracy: 0.9282\n",
            "Epoch 271/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2100 - accuracy: 0.9274\n",
            "Epoch 272/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2035 - accuracy: 0.9298\n",
            "Epoch 273/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.1983 - accuracy: 0.9329\n",
            "Epoch 274/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.1946 - accuracy: 0.9345\n",
            "Epoch 275/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.1944 - accuracy: 0.9329\n",
            "Epoch 276/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.1976 - accuracy: 0.9282\n",
            "Epoch 277/350\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.2048 - accuracy: 0.9266\n",
            "Epoch 278/350\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.1991 - accuracy: 0.9345\n",
            "Epoch 279/350\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.2002 - accuracy: 0.9290\n",
            "Epoch 280/350\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1957 - accuracy: 0.9305\n",
            "Epoch 281/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2009 - accuracy: 0.9329\n",
            "Epoch 282/350\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.2088 - accuracy: 0.9250\n",
            "Epoch 283/350\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.2009 - accuracy: 0.9361\n",
            "Epoch 284/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.1987 - accuracy: 0.9329\n",
            "Epoch 285/350\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.2039 - accuracy: 0.9353\n",
            "Epoch 286/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.1987 - accuracy: 0.9321\n",
            "Epoch 287/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2017 - accuracy: 0.9329\n",
            "Epoch 288/350\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.2011 - accuracy: 0.9329\n",
            "Epoch 289/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.1965 - accuracy: 0.9321\n",
            "Epoch 290/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.1874 - accuracy: 0.9321\n",
            "Epoch 291/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.2017 - accuracy: 0.9298\n",
            "Epoch 292/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.2033 - accuracy: 0.9266\n",
            "Epoch 293/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.1933 - accuracy: 0.9345\n",
            "Epoch 294/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.2004 - accuracy: 0.9337\n",
            "Epoch 295/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.1872 - accuracy: 0.9384\n",
            "Epoch 296/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.1936 - accuracy: 0.9353\n",
            "Epoch 297/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.1949 - accuracy: 0.9321\n",
            "Epoch 298/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.1938 - accuracy: 0.9305\n",
            "Epoch 299/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.1918 - accuracy: 0.9345\n",
            "Epoch 300/350\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1928 - accuracy: 0.9329\n",
            "Epoch 301/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2016 - accuracy: 0.9274\n",
            "Epoch 302/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.1933 - accuracy: 0.9313\n",
            "Epoch 303/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.1923 - accuracy: 0.9329\n",
            "Epoch 304/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.1985 - accuracy: 0.9290\n",
            "Epoch 305/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.1935 - accuracy: 0.9369\n",
            "Epoch 306/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.1882 - accuracy: 0.9321\n",
            "Epoch 307/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.1936 - accuracy: 0.9353\n",
            "Epoch 308/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.1899 - accuracy: 0.9305\n",
            "Epoch 309/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.1903 - accuracy: 0.9313\n",
            "Epoch 310/350\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1952 - accuracy: 0.9345\n",
            "Epoch 311/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.1907 - accuracy: 0.9337\n",
            "Epoch 312/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.1878 - accuracy: 0.9353\n",
            "Epoch 313/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.1916 - accuracy: 0.9321\n",
            "Epoch 314/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.1997 - accuracy: 0.9298\n",
            "Epoch 315/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.1911 - accuracy: 0.9353\n",
            "Epoch 316/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.1922 - accuracy: 0.9305\n",
            "Epoch 317/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.1867 - accuracy: 0.9305\n",
            "Epoch 318/350\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.1885 - accuracy: 0.9392\n",
            "Epoch 319/350\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.1900 - accuracy: 0.9361\n",
            "Epoch 320/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.1905 - accuracy: 0.9369\n",
            "Epoch 321/350\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1838 - accuracy: 0.9361\n",
            "Epoch 322/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.1919 - accuracy: 0.9337\n",
            "Epoch 323/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.1878 - accuracy: 0.9329\n",
            "Epoch 324/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.1865 - accuracy: 0.9345\n",
            "Epoch 325/350\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.1829 - accuracy: 0.9353\n",
            "Epoch 326/350\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.1870 - accuracy: 0.9353\n",
            "Epoch 327/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.1853 - accuracy: 0.9345\n",
            "Epoch 328/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.1893 - accuracy: 0.9337\n",
            "Epoch 329/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.1854 - accuracy: 0.9361\n",
            "Epoch 330/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.1872 - accuracy: 0.9392\n",
            "Epoch 331/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.1873 - accuracy: 0.9345\n",
            "Epoch 332/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.1808 - accuracy: 0.9361\n",
            "Epoch 333/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.1879 - accuracy: 0.9321\n",
            "Epoch 334/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.1878 - accuracy: 0.9321\n",
            "Epoch 335/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.1820 - accuracy: 0.9337\n",
            "Epoch 336/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.1865 - accuracy: 0.9305\n",
            "Epoch 337/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.1867 - accuracy: 0.9369\n",
            "Epoch 338/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.1891 - accuracy: 0.9321\n",
            "Epoch 339/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.1847 - accuracy: 0.9345\n",
            "Epoch 340/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.1869 - accuracy: 0.9353\n",
            "Epoch 341/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.1834 - accuracy: 0.9392\n",
            "Epoch 342/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.1940 - accuracy: 0.9242\n",
            "Epoch 343/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.1821 - accuracy: 0.9369\n",
            "Epoch 344/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.1819 - accuracy: 0.9353\n",
            "Epoch 345/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.1814 - accuracy: 0.9329\n",
            "Epoch 346/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.1876 - accuracy: 0.9345\n",
            "Epoch 347/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.1837 - accuracy: 0.9369\n",
            "Epoch 348/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.1848 - accuracy: 0.9329\n",
            "Epoch 349/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.1902 - accuracy: 0.9290\n",
            "Epoch 350/350\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.1871 - accuracy: 0.9353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.metrics import accuracy\n",
        "pred = classifier.predict(X_test)\n",
        "pred_ = np.argmax(pred, axis = 1)\n",
        "pred_ = l_encode.inverse_transform(pred_)\n",
        "\n",
        "true_y = l_encode.inverse_transform(np.argmax(to_categorical(y_test), axis = 1)[:,1])\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "cm = confusion_matrix(true_y, pred_, labels=[0, 1, 2])\n",
        "cmd = ConfusionMatrixDisplay(cm, display_labels=['easy','moderate/hard', 'very hard'])\n",
        "cmd.plot()\n",
        "\n",
        "cnt = 0\n",
        "cnt_correct = 0\n",
        "for i in range(len(pred)):\n",
        "  cnt += 1\n",
        "  if(pred_[i] == true_y[i]):\n",
        "    cnt_correct += 1\n",
        "print(f'accuracy: {(cnt_correct/cnt)*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "uddqepBt5EkP",
        "outputId": "cd9f7fdb-8ec8-40f9-bc7c-36b8ab589277"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 90.38%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = ['n', 'k', 'n/m', 'av.length', 'std.dev', 'y']\n",
        "df_reg = pd.DataFrame(columns = column_names)\n",
        "print(X_test_copy.shape[0])\n",
        "print(len(pred))\n",
        "for i in range(len(pred)):\n",
        "  if(pred_[i] == 1):\n",
        "    dictionary = {'n' : X_test_copy.iloc[i]['n'], 'n/m': 2, \n",
        "                  'av.length' : X_test_copy.iloc[i]['av.length'], 'std.dev' : X_test_copy.iloc[i]['std.dev'],\n",
        "                  'k' : X_test_copy.iloc[i]['k'], 'y': X_test_copy.iloc[i]['y']}\n",
        "    df_reg = df_reg.append(dictionary, ignore_index = True)\n",
        "#VM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIbvATDjCLTq",
        "outputId": "b521a741-93d1-4af1-863e-72e37409a220"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "260\n",
            "260\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = ['n', 'k', 'm', 'std.dev', 'y']\n",
        "df_reg0 = pd.DataFrame(columns = column_names)\n",
        "print(X_test_copy.shape[0])\n",
        "print(len(pred))\n",
        "for i in range(len(pred)):\n",
        "  if(pred_[i] == 0):\n",
        "    dictionary = {'n' : X_test_copy.iloc[i]['n'], 'max': X_test_copy.iloc[i]['max'],\n",
        "                  'm' : X_test_copy.iloc[i]['m'], 'std.dev' : X_test_copy.iloc[i]['std.dev'],\n",
        "                  'k' : X_test_copy.iloc[i]['k'], 'y': X_test_copy.iloc[i]['y']}\n",
        "    df_reg0 = df_reg0.append(dictionary, ignore_index = True)\n",
        "#EASY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tqrtKj1VBkA",
        "outputId": "04a915ca-7ec5-46e6-f11c-362be8b64e7c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "260\n",
            "260\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = ['n', 'k', 'av.length', 'std.dev', 'm', 'y']\n",
        "df_reg2 = pd.DataFrame(columns = column_names)\n",
        "print(X_test_copy.shape[0])\n",
        "print(len(pred))\n",
        "for i in range(len(pred)):\n",
        "  if(pred_[i] == 2):\n",
        "    dictionary = {'n' : X_test_copy.iloc[i]['n'], 'm': X_test_copy.iloc[i]['m'],\n",
        "                  'av.length' : X_test_copy.iloc[i]['av.length'], 'std.dev' : X_test_copy.iloc[i]['std.dev'],\n",
        "                  'k' : X_test_copy.iloc[i]['k'], 'y': X_test_copy.iloc[i]['y']}\n",
        "    df_reg2 = df_reg2.append(dictionary, ignore_index = True)\n",
        "\n",
        "#VHARD"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIEyHIVPVvyU",
        "outputId": "c1c65d0d-ae16-4f27-e160-4b529587e73e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "260\n",
            "260\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#IN PROGRESS....\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "import time\n",
        "import keras\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/sample_data/C-instances-runtime-analysis.csv')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df = df[df['n/m']!=2]\n",
        "\n",
        "shuffled = df.sample(frac=1).reset_index()\n",
        "\n",
        "columns_dont_want = [ 'index', 'inst.name','type', 'CPLEXStatus'] \n",
        "select = [x for x in shuffled.columns if x not in columns_dont_want]\n",
        "\n",
        "X = shuffled.loc[:, select]\n",
        "\n",
        "y = shuffled.loc[:,'y']\n",
        "names = ['indeks', 'n', 'n/m', '(n/m)^2', '(n/m)^3', 'm/n', 'av.length', 'std.dev', 'median', 'range', 'min', 'max', 'k']"
      ],
      "metadata": {
        "id": "P9a6vACH7zLA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_new = []\n",
        "for val in y:\n",
        "  if(val<10):\n",
        "    y_new.append(0)\n",
        "  elif(10 <= val and val <1000):\n",
        "    y_new.append(1)\n",
        "  else:\n",
        "    y_new.append(2)\n",
        "\n",
        "shuffled['y_new'] = y_new\n",
        "y_encoded = shuffled.loc[:,'y_new']\n",
        "y_new = np.array(y_new)\n",
        "X_modified = X"
      ],
      "metadata": {
        "id": "Mt9OHP2u9xhR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "names =[ 'std.dev', 'n','n/m', 'max','av.length', 'm', 'k']\n",
        "X_modified = X.drop([  'median', 'range',  'min','indeks', 'class',  'subtype', '(m/n)^3', '(m/n)^2', '(n/m)^2', '(n/m)^3','m/n'], axis = 1)\n",
        "X_modified['n/m'] = X_modified['n/m']\n",
        "#print(X_modified.head())\n",
        "\n",
        "oversample = RandomOverSampler(random_state=0)\n",
        "#oversample.fit(X_modified, y_encoded)\n",
        "X_modified, y_encoded = oversample.fit_resample(X_modified, y_encoded)"
      ],
      "metadata": {
        "id": "IGD1S2VI91L8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "l_encode = LabelEncoder()\n",
        "l_encode.fit(y_encoded)\n",
        "y_encoded = l_encode.transform(y_encoded)\n",
        "y_encoded = to_categorical(y_encoded)\n",
        "y_encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdnbu1X494sV",
        "outputId": "b4879a48-0d9a-4196-905f-f67dbc5ea7b0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       ...,\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc = StandardScaler()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_modified, y_encoded, random_state=0, train_size = 0.83)\n",
        "print(X_train.shape[0])\n",
        "print(X_test.shape[0])\n",
        "\n",
        "X_train = X_train.drop(['y',  'm'], axis = 1)\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test_copy = X_test\n",
        "print(X_test_copy.head())\n",
        "X_test = X_test.drop(['y',  'm'], axis = 1)\n",
        "X_test = sc.transform(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlUE_owL98c2",
        "outputId": "f8023b74-1164-417f-bb44-1f8a876bdd9b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11401\n",
            "2336\n",
            "         n   m   n/m   av.length     std.dev  max    k             y\n",
            "3234   160  40  4.00   53.724998   29.127333  100   78     40.832211\n",
            "4495   198  72  2.75  508.691925  167.474625  790  172  99999.000000\n",
            "10982   36  16  2.25   52.750000   24.797897   97   32      1.885425\n",
            "9657    18   8  2.25   74.722221   16.876038  101   16      0.093937\n",
            "5976   162  54  3.00   46.740742   28.289444  100   76     41.455616\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "\n",
        "classifier = Sequential()\n",
        "classifier.add(Dense(16, input_dim = X_modified.shape[1]-2, activation='relu'))\n",
        "\n",
        "classifier.add(Dropout(0.2))\n",
        "\n",
        "classifier.add(Dense(3, activation = 'softmax'))\n",
        "classifier.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "classifier.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvhqjpBH9_u0",
        "outputId": "6635792b-4cf2-4875-a697-7061766aaf7d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_3 (Dense)             (None, 16)                112       \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 3)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 163\n",
            "Trainable params: 163\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.metrics import accuracy\n",
        "from keras import callbacks\n",
        "earlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", \n",
        "                                        mode =\"min\", patience = 25, \n",
        "                                        restore_best_weights = True)\n",
        "  \n",
        "history = classifier.fit(X_train, y_train, batch_size = 32, \n",
        "                    epochs = 120)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh4s76I9-FP2",
        "outputId": "1e7e8fc7-e302-4779-8716-9f66065702ec"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.7257 - accuracy: 0.6885\n",
            "Epoch 2/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.4566 - accuracy: 0.8214\n",
            "Epoch 3/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.4088 - accuracy: 0.8414\n",
            "Epoch 4/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.3889 - accuracy: 0.8469\n",
            "Epoch 5/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.3691 - accuracy: 0.8519\n",
            "Epoch 6/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.3596 - accuracy: 0.8618\n",
            "Epoch 7/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.3525 - accuracy: 0.8613\n",
            "Epoch 8/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.3413 - accuracy: 0.8653\n",
            "Epoch 9/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.3447 - accuracy: 0.8650\n",
            "Epoch 10/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.3382 - accuracy: 0.8684\n",
            "Epoch 11/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.3318 - accuracy: 0.8706\n",
            "Epoch 12/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.3302 - accuracy: 0.8726\n",
            "Epoch 13/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.3239 - accuracy: 0.8747\n",
            "Epoch 14/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.3226 - accuracy: 0.8739\n",
            "Epoch 15/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.3130 - accuracy: 0.8753\n",
            "Epoch 16/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.3133 - accuracy: 0.8775\n",
            "Epoch 17/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.3135 - accuracy: 0.8771\n",
            "Epoch 18/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.3108 - accuracy: 0.8802\n",
            "Epoch 19/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.3054 - accuracy: 0.8816\n",
            "Epoch 20/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.3075 - accuracy: 0.8799\n",
            "Epoch 21/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.3051 - accuracy: 0.8820\n",
            "Epoch 22/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.3025 - accuracy: 0.8806\n",
            "Epoch 23/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.3026 - accuracy: 0.8839\n",
            "Epoch 24/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2949 - accuracy: 0.8840\n",
            "Epoch 25/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.3006 - accuracy: 0.8860\n",
            "Epoch 26/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2944 - accuracy: 0.8819\n",
            "Epoch 27/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2937 - accuracy: 0.8846\n",
            "Epoch 28/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2966 - accuracy: 0.8829\n",
            "Epoch 29/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2923 - accuracy: 0.8859\n",
            "Epoch 30/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2869 - accuracy: 0.8854\n",
            "Epoch 31/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2872 - accuracy: 0.8877\n",
            "Epoch 32/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2889 - accuracy: 0.8849\n",
            "Epoch 33/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2859 - accuracy: 0.8881\n",
            "Epoch 34/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2797 - accuracy: 0.8881\n",
            "Epoch 35/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2845 - accuracy: 0.8888\n",
            "Epoch 36/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2832 - accuracy: 0.8870\n",
            "Epoch 37/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2836 - accuracy: 0.8873\n",
            "Epoch 38/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2822 - accuracy: 0.8879\n",
            "Epoch 39/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2824 - accuracy: 0.8894\n",
            "Epoch 40/120\n",
            "357/357 [==============================] - 1s 1ms/step - loss: 0.2776 - accuracy: 0.8894\n",
            "Epoch 41/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2763 - accuracy: 0.8907\n",
            "Epoch 42/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2811 - accuracy: 0.8871\n",
            "Epoch 43/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2831 - accuracy: 0.8882\n",
            "Epoch 44/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2775 - accuracy: 0.8909\n",
            "Epoch 45/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2732 - accuracy: 0.8911\n",
            "Epoch 46/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2708 - accuracy: 0.8911\n",
            "Epoch 47/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2758 - accuracy: 0.8911\n",
            "Epoch 48/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2716 - accuracy: 0.8904\n",
            "Epoch 49/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2713 - accuracy: 0.8913\n",
            "Epoch 50/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2755 - accuracy: 0.8882\n",
            "Epoch 51/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2653 - accuracy: 0.8963\n",
            "Epoch 52/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2761 - accuracy: 0.8913\n",
            "Epoch 53/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2687 - accuracy: 0.8917\n",
            "Epoch 54/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2701 - accuracy: 0.8897\n",
            "Epoch 55/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2712 - accuracy: 0.8938\n",
            "Epoch 56/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2683 - accuracy: 0.8906\n",
            "Epoch 57/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2704 - accuracy: 0.8878\n",
            "Epoch 58/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2656 - accuracy: 0.8919\n",
            "Epoch 59/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2681 - accuracy: 0.8939\n",
            "Epoch 60/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2665 - accuracy: 0.8905\n",
            "Epoch 61/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2652 - accuracy: 0.8919\n",
            "Epoch 62/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2683 - accuracy: 0.8932\n",
            "Epoch 63/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2692 - accuracy: 0.8886\n",
            "Epoch 64/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2686 - accuracy: 0.8911\n",
            "Epoch 65/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2697 - accuracy: 0.8915\n",
            "Epoch 66/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2677 - accuracy: 0.8916\n",
            "Epoch 67/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2666 - accuracy: 0.8931\n",
            "Epoch 68/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2656 - accuracy: 0.8933\n",
            "Epoch 69/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2674 - accuracy: 0.8897\n",
            "Epoch 70/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2667 - accuracy: 0.8929\n",
            "Epoch 71/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2653 - accuracy: 0.8919\n",
            "Epoch 72/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2626 - accuracy: 0.8940\n",
            "Epoch 73/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2663 - accuracy: 0.8936\n",
            "Epoch 74/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2695 - accuracy: 0.8932\n",
            "Epoch 75/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2638 - accuracy: 0.8923\n",
            "Epoch 76/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2669 - accuracy: 0.8940\n",
            "Epoch 77/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2677 - accuracy: 0.8896\n",
            "Epoch 78/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2662 - accuracy: 0.8935\n",
            "Epoch 79/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2666 - accuracy: 0.8920\n",
            "Epoch 80/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2639 - accuracy: 0.8918\n",
            "Epoch 81/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2633 - accuracy: 0.8923\n",
            "Epoch 82/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2674 - accuracy: 0.8924\n",
            "Epoch 83/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2652 - accuracy: 0.8903\n",
            "Epoch 84/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2630 - accuracy: 0.8910\n",
            "Epoch 85/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2630 - accuracy: 0.8916\n",
            "Epoch 86/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2644 - accuracy: 0.8926\n",
            "Epoch 87/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2574 - accuracy: 0.8949\n",
            "Epoch 88/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2642 - accuracy: 0.8919\n",
            "Epoch 89/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2643 - accuracy: 0.8926\n",
            "Epoch 90/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2605 - accuracy: 0.8947\n",
            "Epoch 91/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2624 - accuracy: 0.8991\n",
            "Epoch 92/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2622 - accuracy: 0.8942\n",
            "Epoch 93/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2633 - accuracy: 0.8946\n",
            "Epoch 94/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2608 - accuracy: 0.8967\n",
            "Epoch 95/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2652 - accuracy: 0.8925\n",
            "Epoch 96/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2653 - accuracy: 0.8923\n",
            "Epoch 97/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2608 - accuracy: 0.8953\n",
            "Epoch 98/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2690 - accuracy: 0.8931\n",
            "Epoch 99/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2649 - accuracy: 0.8924\n",
            "Epoch 100/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2622 - accuracy: 0.8938\n",
            "Epoch 101/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2608 - accuracy: 0.8954\n",
            "Epoch 102/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2657 - accuracy: 0.8931\n",
            "Epoch 103/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2594 - accuracy: 0.8929\n",
            "Epoch 104/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2661 - accuracy: 0.8917\n",
            "Epoch 105/120\n",
            "357/357 [==============================] - 1s 3ms/step - loss: 0.2636 - accuracy: 0.8939\n",
            "Epoch 106/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2596 - accuracy: 0.8971\n",
            "Epoch 107/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2614 - accuracy: 0.8921\n",
            "Epoch 108/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2628 - accuracy: 0.8934\n",
            "Epoch 109/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2641 - accuracy: 0.8941\n",
            "Epoch 110/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2635 - accuracy: 0.8924\n",
            "Epoch 111/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2594 - accuracy: 0.8939\n",
            "Epoch 112/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2607 - accuracy: 0.8936\n",
            "Epoch 113/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2584 - accuracy: 0.8942\n",
            "Epoch 114/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2589 - accuracy: 0.8970\n",
            "Epoch 115/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2603 - accuracy: 0.8933\n",
            "Epoch 116/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2578 - accuracy: 0.8955\n",
            "Epoch 117/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2552 - accuracy: 0.8963\n",
            "Epoch 118/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2583 - accuracy: 0.8940\n",
            "Epoch 119/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2592 - accuracy: 0.8969\n",
            "Epoch 120/120\n",
            "357/357 [==============================] - 1s 2ms/step - loss: 0.2552 - accuracy: 0.8963\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.metrics import accuracy\n",
        "pred = classifier.predict(X_test)\n",
        "pred_ = np.argmax(pred, axis = 1)\n",
        "pred_ = l_encode.inverse_transform(pred_)\n",
        "\n",
        "true_y = l_encode.inverse_transform(np.argmax(to_categorical(y_test), axis = 1)[:,1])\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "cm = confusion_matrix(true_y, pred_, labels=[0, 1, 2])\n",
        "cmd = ConfusionMatrixDisplay(cm, display_labels=['easy','moderate/hard', 'very hard'])\n",
        "\n",
        "cmd.plot()\n",
        "\n",
        "cnt = 0\n",
        "cnt_correct = 0\n",
        "for i in range(len(pred)):\n",
        "  cnt += 1\n",
        "  if(pred_[i] == true_y[i]):\n",
        "    cnt_correct += 1\n",
        "print(f'accuracy: {(cnt_correct/cnt)*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "RYneViFj-F05",
        "outputId": "8a108e33-2007-4de1-e417-993585b380bd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 91.18%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(pred)):\n",
        "  if(pred_[i] == 1):\n",
        "    dictionary = {'n' : X_test_copy.iloc[i]['n'], 'n/m': X_test_copy.iloc[i]['n/m'], \n",
        "                  'av.length' : X_test_copy.iloc[i]['av.length'], 'std.dev' : X_test_copy.iloc[i]['std.dev'],\n",
        "                  'k' : X_test_copy.iloc[i]['k'], 'y': X_test_copy.iloc[i]['y']}\n",
        "    df_reg = df_reg.append(dictionary, ignore_index = True)\n",
        "\n",
        "for i in range(len(pred)):\n",
        "  if(pred_[i] == 0):\n",
        "    dictionary = {'n' : X_test_copy.iloc[i]['n'], 'max': X_test_copy.iloc[i]['max'],\n",
        "                  'm' : X_test_copy.iloc[i]['m'], 'std.dev' : X_test_copy.iloc[i]['std.dev'],\n",
        "                  'k' : X_test_copy.iloc[i]['k'], 'y': X_test_copy.iloc[i]['y']}\n",
        "    df_reg0 = df_reg0.append(dictionary, ignore_index = True)\n",
        "#EASY\n",
        "\n",
        "for i in range(len(pred)):\n",
        "  if(pred_[i] == 2):\n",
        "    dictionary = {'n' : X_test_copy.iloc[i]['n'], 'm': X_test_copy.iloc[i]['m'],\n",
        "                  'av.length' : X_test_copy.iloc[i]['av.length'], 'std.dev' : X_test_copy.iloc[i]['std.dev'],\n",
        "                  'k' : X_test_copy.iloc[i]['k'], 'y': X_test_copy.iloc[i]['y']}\n",
        "    df_reg2 = df_reg2.append(dictionary, ignore_index = True)\n",
        "\n",
        "#VHARD"
      ],
      "metadata": {
        "id": "Gbdkp-4CAIgr"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_reg.shape[0])\n",
        "print(df_reg.head(500))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvWFrDKjDsB2",
        "outputId": "ad421d48-9483-4088-8549-a3b8bd90e81b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "822\n",
            "         n      k    n/m   av.length     std.dev          y\n",
            "0    180.0   75.0   2.00   59.994446   23.497177  22.163019\n",
            "1    160.0   49.0   2.00   73.662498   14.831930  10.595284\n",
            "2    120.0   68.0   2.00   51.858334   29.652130  14.296589\n",
            "3    200.0   86.0   2.00   47.115002   30.439852   2.294968\n",
            "4    200.0  170.0   2.00  477.850006  163.834488  65.560165\n",
            "..     ...    ...    ...         ...         ...        ...\n",
            "495  162.0  108.0   2.25  104.382713   49.349834   4.153656\n",
            "496  140.0   60.0   5.00   99.935715   19.174026  49.476032\n",
            "497  144.0   97.0   2.25  103.076385   54.047779   3.564702\n",
            "498  100.0   65.0  10.00   47.400002   28.686743   9.583375\n",
            "499  180.0   81.0   2.50   96.805557   21.808741  51.911041\n",
            "\n",
            "[500 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_reg0.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmTqU4gCd7qy",
        "outputId": "668e5e3c-0e8f-4ada-cd59-6afeca200cf8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       n      k     m     std.dev          y     max\n",
            "0   80.0   56.0  40.0   26.850121  11.129904   100.0\n",
            "1   40.0   31.0  20.0   22.303143   0.654424   160.0\n",
            "2  140.0  127.0  70.0  133.063202   2.286228   914.0\n",
            "3   80.0   39.0  40.0   15.746810   2.160869   100.0\n",
            "4  180.0  159.0  90.0  185.726913   6.023255  1266.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_reg2.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OY4fn082d-OD",
        "outputId": "f847af82-d3c2-43c0-f285-404b4de764da"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       n      k   av.length     std.dev     m             y\n",
            "0  180.0  154.0  706.877808  175.294418  90.0  99999.000000\n",
            "1  120.0  107.0  301.091675  102.802505  60.0      3.469282\n",
            "2   80.0   66.0  106.937500   48.509495  40.0  99999.000000\n",
            "3  140.0  121.0  547.578552  137.977631  70.0  99999.000000\n",
            "4  120.0  100.0  319.049988  109.162712  60.0  99999.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(df_reg.shape[0]):\n",
        "  if(df_reg.iloc[i]['y'] >1000):\n",
        "    df_reg.loc[i, 'y'] = 1001\n",
        "\n",
        "for i in range(df_reg0.shape[0]):\n",
        "  if(df_reg0.iloc[i]['y'] > 10):\n",
        "    df_reg0.loc[i, 'y'] = 11\n",
        "\n",
        "for i in range(df_reg2.shape[0]):\n",
        "  if(df_reg2.iloc[i]['y'] == 99999):\n",
        "    df_reg2.loc[i, 'y'] = 4510\n",
        "\n",
        "\n",
        "print(df_reg['y'].max())\n",
        "shuffled = df_reg.sample(frac=1).reset_index()\n",
        "\n",
        "columns_dont_want = ['y', 'index', 'inst.name','type', 'CPLEXStatus']\n",
        "select = [x for x in shuffled.columns if x not in columns_dont_want]\n",
        "\n",
        "X = shuffled.loc[:, select]\n",
        "\n",
        "y = shuffled.loc[:,'y']\n",
        "X_modified = X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STxF3c_kMGtp",
        "outputId": "5cbfebe4-a32e-4bca-8508-46cff73afa1f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1001.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "oversample = RandomOverSampler(random_state=0)\n",
        "X_modified, y = oversample.fit_resample(X_modified, y.astype('int'))\n",
        "print(X_modified.shape[0])\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote_on_3 = 50\n",
        "#oversample = SMOTE(sampling_strategy={99999:50000})\n",
        "#X_modified, y = oversample.fit_resample(X_modified, y)\n",
        "print(X_modified.shape[0])\n",
        "\n",
        "tmp = X_modified\n",
        "tmp['y'] = y\n",
        "tmp = tmp.sample(frac = 1).reset_index()\n",
        "print(tmp.head(60))\n",
        "#y = np.log10(tmp['y']*10)+20\n",
        "y = tmp['y']\n",
        "X_modified = tmp.drop(['y'], axis = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbZsImk5MaVQ",
        "outputId": "4742bbd6-18d0-4222-dbfc-588447b91ceb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6312\n",
            "6312\n",
            "    index      n      k    n/m   av.length     std.dev    y\n",
            "0    1783  180.0  109.0   3.00  103.933334   49.045372   60\n",
            "1    3237  108.0   56.0   9.00   61.462963   24.649858  134\n",
            "2    6137   80.0   40.0  10.00   73.574997   14.510385  751\n",
            "3    5728  162.0   70.0   6.00   58.728394   22.805967  491\n",
            "4    3045  120.0   71.0   5.00   47.933334   29.299019  123\n",
            "5    5363   90.0   56.0   6.00   96.633331   21.565914  357\n",
            "6    2655  200.0   88.0   5.00   50.060001   30.199945  105\n",
            "7    1472  162.0   66.0   2.25  101.012344   18.443462   44\n",
            "8    4748  140.0  119.0   2.50  332.799988  112.981964  273\n",
            "9    3924   60.0   49.0   5.00  147.333328   55.840767  187\n",
            "10   2758  144.0   71.0   4.50  101.243057   20.468037  110\n",
            "11   5520  110.0   95.0   2.75  440.518188  114.116776  411\n",
            "12   4909  160.0   47.0   5.00   74.793747   15.111367  289\n",
            "13   1325   80.0   68.0   2.50  322.787506   81.723465   35\n",
            "14   4508   40.0   38.0  10.00  159.175003   37.038364  247\n",
            "15   3130  180.0   65.0   4.50  101.433334   17.105360  127\n",
            "16   2772  144.0   71.0   4.50  101.243057   20.468037  110\n",
            "17   2941  108.0   63.0   9.00   59.462963   24.030130  119\n",
            "18   5555  162.0   71.0   6.00   57.469135   24.604242  437\n",
            "19   5381  160.0   66.0   5.00   57.362499   22.339712  364\n",
            "20   1692  126.0   61.0   3.00   61.293652   23.535868   55\n",
            "21   5530  126.0   95.0   4.50  103.984123   48.371731  418\n",
            "22   1396  140.0   47.0   5.00   74.707146   13.124470   39\n",
            "23   5935  162.0   70.0   6.00   59.191357   23.979164  562\n",
            "24   3553   54.0   46.0   6.00  219.351852   58.945820  157\n",
            "25    983  180.0   50.0   2.00   75.622223   14.227270   12\n",
            "26   1000  108.0   65.0   2.25   47.777779   29.947725   14\n",
            "27   2118  132.0   69.0   2.75   99.666664   20.291636   77\n",
            "28   3069  108.0   63.0   6.00   59.462963   24.030130  124\n",
            "29   3254  160.0  110.0   4.00  100.406250   47.475151  135\n",
            "30   3507   60.0   44.0  10.00   49.233334   27.297709  155\n",
            "31   2256  110.0   81.0   2.75   98.472725   47.872105   84\n",
            "32   1773  180.0  109.0   3.00  103.933334   49.045372   60\n",
            "33   1705   36.0   26.0   9.00   72.833336   15.155385   56\n",
            "34    723  108.0   66.0   4.50   51.277779   27.951508  128\n",
            "35   6160  108.0   56.0   6.00  100.907410   18.343826  812\n",
            "36   6059  144.0   62.0   6.00   97.152779   18.898079  649\n",
            "37   6096  180.0   74.0   6.00   58.255554   23.915146  695\n",
            "38   1893  200.0   78.0   5.00   60.630001   23.761440   66\n",
            "39    480  108.0   59.0   4.50   58.824074   23.824841   33\n",
            "40    650  120.0   68.0   2.00   59.224998   22.930464   11\n",
            "41   3144  108.0   66.0   4.50   51.277779   27.951508  128\n",
            "42   5058   90.0   78.0   3.00  378.444458   83.558784  315\n",
            "43   2094  220.0   51.0   2.75   73.995453   15.305265   76\n",
            "44    648  144.0   97.0   2.25  103.076385   54.047779    3\n",
            "45   5562  162.0   71.0   6.00   57.469135   24.604242  437\n",
            "46   2569   54.0   49.0   4.50   94.833336   54.929371   99\n",
            "47   1591  144.0   71.0   3.00  101.243057   20.468037   50\n",
            "48   4358  126.0   99.0   3.00  307.134918  106.701698  232\n",
            "49   3533   72.0   50.0   9.00   53.569443   31.970419  156\n",
            "50   4404  100.0   50.0  10.00   59.610001   24.303177  238\n",
            "51   1805  162.0   69.0   3.00   96.320984   19.719355   61\n",
            "52   3637   60.0   45.0  10.00   49.150002   29.713789  166\n",
            "53    274  162.0   47.0   4.50   75.037041   14.629539  302\n",
            "54   3092  140.0   99.0   5.00  100.471428   47.371552  125\n",
            "55   1879  200.0   51.0   2.50   75.389999   15.575004   65\n",
            "56   1528  110.0   57.0   2.75  100.000000   18.774372   47\n",
            "57   1090   60.0   50.0   5.00  242.500000   63.423115   21\n",
            "58   4134   60.0   53.0   5.00  238.516663   63.221569  212\n",
            "59   4502   40.0   38.0  10.00  159.175003   37.038364  247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense ,Dropout,BatchNormalization\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from keras.layers import Activation\n",
        "from keras import backend as K\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras \n",
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "\n",
        "def custom_activation(x):\n",
        "    #return K.relu(tensorflow.subtract(x,-1)) - K.relu(tensorflow.subtract(x,3.5))\n",
        "    return 4510*1/(1+K.exp(-x))\n",
        "\n",
        "\n",
        "get_custom_objects().update({'custom_activation': Activation(custom_activation)})\n",
        "\n",
        "def make_model():\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Dense(8, input_dim=X_modified.shape[1], activation='relu'))\n",
        "    #model.add(Dropout(0.25))\n",
        "    \n",
        "    model.add(Dense(32, activation = 'relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "\n",
        "    model.add(Dense(1, activation = custom_activation))\n",
        "    model.compile(loss='mean_squared_error', optimizer=\"Adam\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "QAi9GPfiMqOX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc = StandardScaler()\n",
        "sc2 = StandardScaler()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_modified, y, random_state=0, train_size = 0.83)\n",
        "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=0, train_size = 0.83)\n",
        "\n",
        "\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test_copy = X_test\n",
        "X_test = sc.transform(X_test)\n",
        "#X_val = sc.transform(X_val)\n",
        "\n",
        "#y_train = sc2.fit_transform(y_train.values.reshape(-1,1))\n",
        "#y_test = sc2.transform(y_test.values.reshape(-1,1))\n",
        "\n",
        "estimator = KerasRegressor(build_fn=make_model, epochs=300, batch_size=64, verbose=1)\n",
        "\n",
        "#history=estimator.fit(X_train,y_train)\n",
        "from keras.metrics import accuracy\n",
        "from keras import callbacks\n",
        "earlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", \n",
        "                                        mode =\"min\", patience = 5, \n",
        "                                        restore_best_weights = True)\n",
        "  \n",
        "history = estimator.fit(X_train, y_train, shuffle = True)\n",
        "#plt.plot(history.history['val_accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqRmSfnjNDi3",
        "outputId": "6952ba48-e5f0-4038-d07f-d85d7bafc3fc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "82/82 [==============================] - 1s 2ms/step - loss: 2741741.5000\n",
            "Epoch 2/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 510046.1875\n",
            "Epoch 3/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 109634.3828\n",
            "Epoch 4/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 63492.0703\n",
            "Epoch 5/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 50142.3867\n",
            "Epoch 6/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 41937.9883\n",
            "Epoch 7/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 35728.5859\n",
            "Epoch 8/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 31357.5312\n",
            "Epoch 9/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 27757.1777\n",
            "Epoch 10/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 25303.9902\n",
            "Epoch 11/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 23037.7266\n",
            "Epoch 12/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 21790.1953\n",
            "Epoch 13/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 20139.4297\n",
            "Epoch 14/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 19775.9258\n",
            "Epoch 15/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 18912.4941\n",
            "Epoch 16/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 18138.9316\n",
            "Epoch 17/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 17690.4961\n",
            "Epoch 18/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 17472.7988\n",
            "Epoch 19/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 17066.4336\n",
            "Epoch 20/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 16575.7852\n",
            "Epoch 21/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 15814.5547\n",
            "Epoch 22/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 16208.7988\n",
            "Epoch 23/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 15398.9707\n",
            "Epoch 24/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 14732.8643\n",
            "Epoch 25/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 14742.1104\n",
            "Epoch 26/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 14357.7734\n",
            "Epoch 27/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 14230.9600\n",
            "Epoch 28/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 13840.7910\n",
            "Epoch 29/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 13432.5205\n",
            "Epoch 30/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 14125.5029\n",
            "Epoch 31/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 13354.7139\n",
            "Epoch 32/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 12891.1201\n",
            "Epoch 33/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 12942.7842\n",
            "Epoch 34/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 12665.5020\n",
            "Epoch 35/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 12324.6738\n",
            "Epoch 36/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 12283.3555\n",
            "Epoch 37/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 12079.3301\n",
            "Epoch 38/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 12246.1660\n",
            "Epoch 39/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 11230.4326\n",
            "Epoch 40/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 11614.9717\n",
            "Epoch 41/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 11179.8096\n",
            "Epoch 42/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 11013.9355\n",
            "Epoch 43/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 11193.1318\n",
            "Epoch 44/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 10906.5264\n",
            "Epoch 45/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 11167.1934\n",
            "Epoch 46/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 10712.1631\n",
            "Epoch 47/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 10529.4316\n",
            "Epoch 48/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 10580.7900\n",
            "Epoch 49/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 10423.3525\n",
            "Epoch 50/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 10322.7852\n",
            "Epoch 51/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 10274.7090\n",
            "Epoch 52/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 10081.6650\n",
            "Epoch 53/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 10162.8428\n",
            "Epoch 54/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 10242.1992\n",
            "Epoch 55/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 10027.6484\n",
            "Epoch 56/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 9945.1396\n",
            "Epoch 57/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 9593.9668\n",
            "Epoch 58/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 9708.9453\n",
            "Epoch 59/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 9660.8623\n",
            "Epoch 60/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 9524.0146\n",
            "Epoch 61/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 9413.3086\n",
            "Epoch 62/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 9296.7588\n",
            "Epoch 63/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 9292.3633\n",
            "Epoch 64/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 9223.7246\n",
            "Epoch 65/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 9207.0762\n",
            "Epoch 66/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 9229.9355\n",
            "Epoch 67/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 9135.2744\n",
            "Epoch 68/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 9047.7227\n",
            "Epoch 69/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 8906.7656\n",
            "Epoch 70/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 9043.2383\n",
            "Epoch 71/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 8802.5430\n",
            "Epoch 72/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 9031.4688\n",
            "Epoch 73/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 8629.1982\n",
            "Epoch 74/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 8707.0820\n",
            "Epoch 75/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 8666.1465\n",
            "Epoch 76/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 8496.1230\n",
            "Epoch 77/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 8325.8760\n",
            "Epoch 78/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 8416.7139\n",
            "Epoch 79/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 8446.6885\n",
            "Epoch 80/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 8414.4980\n",
            "Epoch 81/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 8373.7842\n",
            "Epoch 82/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 8267.9717\n",
            "Epoch 83/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 8389.0400\n",
            "Epoch 84/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 8211.2354\n",
            "Epoch 85/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 7943.0464\n",
            "Epoch 86/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 8283.2109\n",
            "Epoch 87/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 8168.3315\n",
            "Epoch 88/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 7987.7021\n",
            "Epoch 89/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 7918.2261\n",
            "Epoch 90/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 7769.6670\n",
            "Epoch 91/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 7805.0127\n",
            "Epoch 92/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 7586.2734\n",
            "Epoch 93/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 8093.5547\n",
            "Epoch 94/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 7861.2822\n",
            "Epoch 95/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 7780.7842\n",
            "Epoch 96/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 7547.4150\n",
            "Epoch 97/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 7729.4561\n",
            "Epoch 98/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 7510.2964\n",
            "Epoch 99/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 7352.3232\n",
            "Epoch 100/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 7600.1582\n",
            "Epoch 101/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 7251.3584\n",
            "Epoch 102/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 7139.9932\n",
            "Epoch 103/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 6951.8555\n",
            "Epoch 104/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 7096.4268\n",
            "Epoch 105/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 6876.7598\n",
            "Epoch 106/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 6523.7310\n",
            "Epoch 107/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 6660.7817\n",
            "Epoch 108/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 6538.4229\n",
            "Epoch 109/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 6095.4004\n",
            "Epoch 110/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 5982.3389\n",
            "Epoch 111/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 6100.3257\n",
            "Epoch 112/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 6160.7520\n",
            "Epoch 113/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 5772.4263\n",
            "Epoch 114/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 5669.9736\n",
            "Epoch 115/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 5683.6157\n",
            "Epoch 116/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 5600.2969\n",
            "Epoch 117/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 5904.9888\n",
            "Epoch 118/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 5471.7168\n",
            "Epoch 119/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 5460.3511\n",
            "Epoch 120/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 5315.0571\n",
            "Epoch 121/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 5286.9307\n",
            "Epoch 122/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 5166.2212\n",
            "Epoch 123/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 5013.9907\n",
            "Epoch 124/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 5108.0947\n",
            "Epoch 125/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 5086.9106\n",
            "Epoch 126/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 4979.8677\n",
            "Epoch 127/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 5006.3765\n",
            "Epoch 128/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 4992.7031\n",
            "Epoch 129/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 4745.6304\n",
            "Epoch 130/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 4967.0957\n",
            "Epoch 131/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 4839.6172\n",
            "Epoch 132/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 4738.4395\n",
            "Epoch 133/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 4882.2900\n",
            "Epoch 134/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 4883.8340\n",
            "Epoch 135/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 4356.6147\n",
            "Epoch 136/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 4209.7983\n",
            "Epoch 137/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 4458.1353\n",
            "Epoch 138/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 4684.4863\n",
            "Epoch 139/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 4411.9678\n",
            "Epoch 140/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 4341.6294\n",
            "Epoch 141/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 4420.9941\n",
            "Epoch 142/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 4671.8408\n",
            "Epoch 143/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 4503.3950\n",
            "Epoch 144/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 4213.9365\n",
            "Epoch 145/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 4386.6890\n",
            "Epoch 146/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 4223.4844\n",
            "Epoch 147/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 4356.1055\n",
            "Epoch 148/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 4342.9292\n",
            "Epoch 149/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 4060.8323\n",
            "Epoch 150/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 4225.0840\n",
            "Epoch 151/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 4181.5859\n",
            "Epoch 152/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 4465.1084\n",
            "Epoch 153/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 4186.5649\n",
            "Epoch 154/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3975.5059\n",
            "Epoch 155/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 4151.9272\n",
            "Epoch 156/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 4045.3818\n",
            "Epoch 157/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3800.1169\n",
            "Epoch 158/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3895.7500\n",
            "Epoch 159/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 4347.7944\n",
            "Epoch 160/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3839.3569\n",
            "Epoch 161/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3987.1233\n",
            "Epoch 162/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3926.8721\n",
            "Epoch 163/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3650.8831\n",
            "Epoch 164/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3738.5557\n",
            "Epoch 165/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3785.4326\n",
            "Epoch 166/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3614.4868\n",
            "Epoch 167/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3869.2009\n",
            "Epoch 168/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3697.7258\n",
            "Epoch 169/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3515.2053\n",
            "Epoch 170/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3695.7266\n",
            "Epoch 171/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3642.5879\n",
            "Epoch 172/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3432.9595\n",
            "Epoch 173/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3715.1299\n",
            "Epoch 174/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3665.6975\n",
            "Epoch 175/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3950.3567\n",
            "Epoch 176/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3883.6992\n",
            "Epoch 177/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3805.3228\n",
            "Epoch 178/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3719.6846\n",
            "Epoch 179/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3473.2974\n",
            "Epoch 180/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3421.6860\n",
            "Epoch 181/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3737.8713\n",
            "Epoch 182/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3233.3333\n",
            "Epoch 183/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3464.1184\n",
            "Epoch 184/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3498.8042\n",
            "Epoch 185/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3381.7866\n",
            "Epoch 186/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3523.7319\n",
            "Epoch 187/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3453.1631\n",
            "Epoch 188/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3631.5884\n",
            "Epoch 189/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3345.1760\n",
            "Epoch 190/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3495.5601\n",
            "Epoch 191/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3484.9580\n",
            "Epoch 192/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3211.9050\n",
            "Epoch 193/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3798.8855\n",
            "Epoch 194/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3141.7708\n",
            "Epoch 195/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3333.0767\n",
            "Epoch 196/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3276.8918\n",
            "Epoch 197/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3495.1619\n",
            "Epoch 198/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3464.7170\n",
            "Epoch 199/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3352.7942\n",
            "Epoch 200/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3393.2092\n",
            "Epoch 201/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3331.4121\n",
            "Epoch 202/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3207.7913\n",
            "Epoch 203/300\n",
            "82/82 [==============================] - 1s 9ms/step - loss: 3219.8247\n",
            "Epoch 204/300\n",
            "82/82 [==============================] - 0s 5ms/step - loss: 3380.0061\n",
            "Epoch 205/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3288.3474\n",
            "Epoch 206/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3158.9409\n",
            "Epoch 207/300\n",
            "82/82 [==============================] - 0s 5ms/step - loss: 3529.4453\n",
            "Epoch 208/300\n",
            "82/82 [==============================] - 1s 7ms/step - loss: 3059.6267\n",
            "Epoch 209/300\n",
            "82/82 [==============================] - 0s 3ms/step - loss: 2893.3870\n",
            "Epoch 210/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3265.2913\n",
            "Epoch 211/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3132.5598\n",
            "Epoch 212/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3140.7974\n",
            "Epoch 213/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3245.5144\n",
            "Epoch 214/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3309.9961\n",
            "Epoch 215/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3210.5205\n",
            "Epoch 216/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3369.5757\n",
            "Epoch 217/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3124.6985\n",
            "Epoch 218/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3191.9773\n",
            "Epoch 219/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3228.7434\n",
            "Epoch 220/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3329.0203\n",
            "Epoch 221/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 2874.8269\n",
            "Epoch 222/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3027.6731\n",
            "Epoch 223/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3038.7307\n",
            "Epoch 224/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3300.8545\n",
            "Epoch 225/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 2934.0056\n",
            "Epoch 226/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3059.5933\n",
            "Epoch 227/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 2982.8030\n",
            "Epoch 228/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3034.7197\n",
            "Epoch 229/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3286.9324\n",
            "Epoch 230/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3348.8687\n",
            "Epoch 231/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3123.4148\n",
            "Epoch 232/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3239.4873\n",
            "Epoch 233/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3114.8708\n",
            "Epoch 234/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3156.0171\n",
            "Epoch 235/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 2957.9934\n",
            "Epoch 236/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3099.3411\n",
            "Epoch 237/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3040.9929\n",
            "Epoch 238/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 2838.3643\n",
            "Epoch 239/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3146.5913\n",
            "Epoch 240/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 2785.8726\n",
            "Epoch 241/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3029.3032\n",
            "Epoch 242/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 2894.8093\n",
            "Epoch 243/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3012.0310\n",
            "Epoch 244/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3201.0989\n",
            "Epoch 245/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3093.3799\n",
            "Epoch 246/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 2984.2571\n",
            "Epoch 247/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3202.9094\n",
            "Epoch 248/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3052.3376\n",
            "Epoch 249/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3483.5354\n",
            "Epoch 250/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3263.7827\n",
            "Epoch 251/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3023.9658\n",
            "Epoch 252/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 2991.3333\n",
            "Epoch 253/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3032.0781\n",
            "Epoch 254/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 2854.3521\n",
            "Epoch 255/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 2947.8364\n",
            "Epoch 256/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3364.4663\n",
            "Epoch 257/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3070.7673\n",
            "Epoch 258/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3161.2615\n",
            "Epoch 259/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 2925.6318\n",
            "Epoch 260/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3028.7119\n",
            "Epoch 261/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 2953.0471\n",
            "Epoch 262/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3108.9536\n",
            "Epoch 263/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3005.4246\n",
            "Epoch 264/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 2937.9255\n",
            "Epoch 265/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3009.5217\n",
            "Epoch 266/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3331.0933\n",
            "Epoch 267/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3103.7415\n",
            "Epoch 268/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3012.8250\n",
            "Epoch 269/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 2839.4600\n",
            "Epoch 270/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3048.3562\n",
            "Epoch 271/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3332.8850\n",
            "Epoch 272/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3159.6101\n",
            "Epoch 273/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 2843.6912\n",
            "Epoch 274/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 2911.3914\n",
            "Epoch 275/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3111.1499\n",
            "Epoch 276/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3122.7312\n",
            "Epoch 277/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 2880.2749\n",
            "Epoch 278/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 2858.9043\n",
            "Epoch 279/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3050.5125\n",
            "Epoch 280/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 2874.7661\n",
            "Epoch 281/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3498.4224\n",
            "Epoch 282/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3018.8455\n",
            "Epoch 283/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3148.6804\n",
            "Epoch 284/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 2880.0466\n",
            "Epoch 285/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3005.9946\n",
            "Epoch 286/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3113.7712\n",
            "Epoch 287/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3210.0061\n",
            "Epoch 288/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3008.2207\n",
            "Epoch 289/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3005.2373\n",
            "Epoch 290/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 2899.0513\n",
            "Epoch 291/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3013.8906\n",
            "Epoch 292/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3112.4834\n",
            "Epoch 293/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3156.9263\n",
            "Epoch 294/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3008.7139\n",
            "Epoch 295/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3043.1946\n",
            "Epoch 296/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3119.2590\n",
            "Epoch 297/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 2931.8384\n",
            "Epoch 298/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3077.8972\n",
            "Epoch 299/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 2811.3499\n",
            "Epoch 300/300\n",
            "82/82 [==============================] - 0s 2ms/step - loss: 3160.6624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "prediction = estimator.predict(X_test)\n",
        "print(\"r_square score: \", r2_score(y_test,prediction))\n",
        "#y_test = sc2.inverse_transform(y_test.reshape(-1,1))\n",
        "yy = np.array(y_test)\n",
        "#yy = np.power(yy, 10)/10\n",
        "#prediction = sc2.inverse_transform(prediction.reshape(-1,1))\n",
        "predd = np.array(prediction)\n",
        "#predd = np.power(predd, 10)/10\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "print(\"RMSE:\")\n",
        "sqrt(mean_squared_error(yy, predd))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWSUU_tVNL9Y",
        "outputId": "b37aa945-dd15-48e1-afb7-f0511dab2128"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17/17 [==============================] - 0s 1ms/step\n",
            "r_square score:  0.877120485170245\n",
            "RMSE:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.69904119564477"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yy_temp = []\n",
        "predd_temp = []\n",
        "\n",
        "for i in range(len(yy)):\n",
        "  if(yy[i]>=10 and yy[i]<1000):\n",
        "    yy_temp.append(yy[i])\n",
        "    predd_temp.append(predd[i])\n",
        "\n",
        "plt.scatter(yy_temp, predd_temp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "kr8vuMOtNf7g",
        "outputId": "98f0be2c-077b-47b2-ff10-96902ac18838"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f8d9f6a8610>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled = df_reg0.sample(frac=1).reset_index()\n",
        "\n",
        "columns_dont_want = ['y', 'index', 'inst.name','type', 'CPLEXStatus']\n",
        "select = [x for x in shuffled.columns if x not in columns_dont_want]\n",
        "\n",
        "X = shuffled.loc[:, select]\n",
        "\n",
        "y = shuffled.loc[:,'y']\n",
        "X_modified = X"
      ],
      "metadata": {
        "id": "a6jZD4deYNxu"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "oversample = RandomOverSampler(random_state=0)\n",
        "X_modified, y = oversample.fit_resample(X_modified, y.astype('int'))\n",
        "print(X_modified.shape[0])\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote_on_3 = 50\n",
        "#oversample = SMOTE(sampling_strategy={99999:50000})\n",
        "#X_modified, y = oversample.fit_resample(X_modified, y)\n",
        "print(X_modified.shape[0])\n",
        "\n",
        "tmp = X_modified\n",
        "tmp['y'] = y\n",
        "print(tmp.head(60))\n",
        "tmp = tmp.sample(frac = 1).reset_index()\n",
        "#y = np.log10(tmp['y']*10)+20\n",
        "tmp = tmp.dropna()\n",
        "y = tmp['y']\n",
        "X_modified = tmp.drop(['y'], axis = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6TdX9OKYWgi",
        "outputId": "2170d471-0d72-44e1-c57e-f3b6e929ca61"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3542\n",
            "3542\n",
            "       n     k     m    std.dev    max   y\n",
            "0   36.0  29.0   8.0  37.736965  212.0  11\n",
            "1   18.0  16.0   8.0  59.945076  216.0   0\n",
            "2   36.0  31.0  12.0  33.475353  223.0   1\n",
            "3   40.0  29.0   8.0  15.496051  135.0   9\n",
            "4   18.0  17.0   6.0  24.770121   91.0   0\n",
            "5   44.0  39.0  16.0  30.487480   99.0   3\n",
            "6   18.0  16.0   2.0  20.876545  112.0   0\n",
            "7   18.0  18.0   3.0  18.160234  105.0   0\n",
            "8   40.0  33.0  20.0  30.838604  100.0   2\n",
            "9   20.0  16.0   8.0  20.287668   80.0   0\n",
            "10  22.0  21.0   8.0  20.265339   87.0   0\n",
            "11  36.0  32.0  16.0  24.797897   97.0   1\n",
            "12  60.0  44.0  24.0  23.349827  100.0   6\n",
            "13  20.0  19.0   8.0  42.628010  189.0   0\n",
            "14  36.0  31.0  16.0  32.905876  143.0   1\n",
            "15  60.0  53.0  24.0  60.807098  381.0   8\n",
            "16  40.0  37.0  10.0  30.350899  100.0   8\n",
            "17  18.0  15.0   2.0  17.077007   72.0   1\n",
            "18  18.0  18.0   2.0  31.704931   96.0   0\n",
            "19  18.0  15.0   3.0  16.084845  105.0   2\n",
            "20  36.0  34.0   8.0  31.388338  100.0   5\n",
            "21  20.0  20.0   8.0  13.862007   99.0   0\n",
            "22  18.0  17.0   2.0  28.746811   94.0   0\n",
            "23  60.0  46.0  24.0  23.594288   94.0   8\n",
            "24  20.0  17.0   4.0  20.396788   98.0   2\n",
            "25  40.0  32.0   8.0  22.764103  141.0  11\n",
            "26  22.0  20.0   8.0  21.586281  100.0   0\n",
            "27  20.0  19.0  10.0  18.365013  121.0   0\n",
            "28  36.0  31.0  12.0  20.746599  156.0   2\n",
            "29  20.0  17.0   2.0  15.300155   77.0   1\n",
            "30  20.0  19.0   2.0  42.628010  189.0   1\n",
            "31  20.0  19.0  10.0  42.628010  189.0   0\n",
            "32  22.0  20.0   8.0  14.120692   98.0   0\n",
            "33  36.0  32.0   8.0  19.676107  137.0  11\n",
            "34  80.0  55.0  32.0  29.341934   96.0   8\n",
            "35  44.0  31.0  16.0  14.192127   99.0   3\n",
            "36  36.0  24.0  16.0  15.860603  100.0   1\n",
            "37  40.0  27.0  16.0  14.480666  100.0   3\n",
            "38  20.0  16.0   2.0  20.287668   80.0   1\n",
            "39  40.0  33.0  10.0  27.974300   97.0   6\n",
            "40  36.0  34.0  16.0  58.806515  226.0   0\n",
            "41  20.0  20.0   4.0  33.809685  100.0   0\n",
            "42  22.0  19.0   2.0  21.006907  124.0   1\n",
            "43  22.0  20.0   2.0  14.120692   98.0   4\n",
            "44  40.0  36.0  16.0  51.817333  239.0   2\n",
            "45  18.0  16.0   4.0  16.876038  101.0   0\n",
            "46  20.0  20.0   2.0  44.963608  199.0   1\n",
            "47  36.0  32.0  16.0  54.048389  190.0   1\n",
            "48  36.0  35.0  16.0  42.243843  221.0   0\n",
            "49  20.0  19.0   5.0  42.628010  189.0   2\n",
            "50  18.0  15.0   2.0  16.084845  105.0   1\n",
            "51  40.0  35.0  10.0  43.996967  200.0  11\n",
            "52  54.0  46.0  24.0  58.945820  351.0   2\n",
            "53  18.0  16.0   3.0  16.876038  101.0   0\n",
            "54  66.0  49.0  24.0  21.662622  146.0  11\n",
            "55  22.0  21.0   8.0  21.522364   88.0   0\n",
            "56  18.0  15.0   6.0  17.574213  135.0   0\n",
            "57  20.0  17.0   4.0  17.938784  131.0   1\n",
            "58  18.0  17.0   8.0  16.201752  100.0   0\n",
            "59  18.0  16.0   8.0  25.546381   98.0   0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense ,Dropout,BatchNormalization\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from keras.layers import Activation\n",
        "from keras import backend as K\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras \n",
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "\n",
        "def custom_activation(x):\n",
        "    #return K.relu(tensorflow.subtract(x,-1)) - K.relu(tensorflow.subtract(x,3.5))\n",
        "    return 10*1/(1+K.exp(-x))\n",
        "\n",
        "\n",
        "get_custom_objects().update({'custom_activation': Activation(custom_activation)})\n",
        "\n",
        "def make_model():\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Dense(8, input_dim=X_modified.shape[1], activation='relu'))\n",
        "    #model.add(Dropout(0.25))\n",
        "    \n",
        "    model.add(Dense(12, activation = 'relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "\n",
        "    model.add(Dense(1, activation = custom_activation))\n",
        "    model.compile(loss='mean_squared_error', optimizer='Adam')\n",
        "    return model"
      ],
      "metadata": {
        "id": "nn0lOzP3YcUq"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc = StandardScaler()\n",
        "sc2 = StandardScaler()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_modified, y, random_state=0, train_size = 0.83)\n",
        "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=0, train_size = 0.83)\n",
        "\n",
        "\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test_copy = X_test\n",
        "X_test = sc.transform(X_test)\n",
        "#X_val = sc.transform(X_val)\n",
        "\n",
        "#y_train = sc2.fit_transform(y_train.values.reshape(-1,1))\n",
        "#y_test = sc2.transform(y_test.values.reshape(-1,1))\n",
        "\n",
        "estimator = KerasRegressor(build_fn=make_model, epochs=300, batch_size=64, verbose=1)\n",
        "\n",
        "#history=estimator.fit(X_train,y_train)\n",
        "from keras.metrics import accuracy\n",
        "from keras import callbacks\n",
        "earlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", \n",
        "                                        mode =\"min\", patience = 5, \n",
        "                                        restore_best_weights = True)\n",
        "  \n",
        "history = estimator.fit(X_train, y_train, shuffle = True)\n",
        "#plt.plot(history.history['val_accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ms0Uyv45YmQH",
        "outputId": "81f46e83-fb4f-419c-c9f1-38e24dc340ec"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46/46 [==============================] - 1s 2ms/step - loss: 8.0078\n",
            "Epoch 2/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 5.4630\n",
            "Epoch 3/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 4.0441\n",
            "Epoch 4/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 3.3929\n",
            "Epoch 5/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 3.0940\n",
            "Epoch 6/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 2.9879\n",
            "Epoch 7/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 2.8607\n",
            "Epoch 8/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 2.8116\n",
            "Epoch 9/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 2.7015\n",
            "Epoch 10/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 2.5724\n",
            "Epoch 11/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 2.5211\n",
            "Epoch 12/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 2.4045\n",
            "Epoch 13/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 2.3779\n",
            "Epoch 14/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 2.2791\n",
            "Epoch 15/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 2.2664\n",
            "Epoch 16/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 2.2388\n",
            "Epoch 17/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 2.1983\n",
            "Epoch 18/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 2.1643\n",
            "Epoch 19/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 2.1065\n",
            "Epoch 20/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 2.0243\n",
            "Epoch 21/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 2.0231\n",
            "Epoch 22/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.8842\n",
            "Epoch 23/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.9556\n",
            "Epoch 24/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.8493\n",
            "Epoch 25/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.8345\n",
            "Epoch 26/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.8407\n",
            "Epoch 27/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.7298\n",
            "Epoch 28/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.7417\n",
            "Epoch 29/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.7617\n",
            "Epoch 30/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.7720\n",
            "Epoch 31/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.6820\n",
            "Epoch 32/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.7025\n",
            "Epoch 33/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.6073\n",
            "Epoch 34/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.6096\n",
            "Epoch 35/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.6389\n",
            "Epoch 36/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.5735\n",
            "Epoch 37/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.5875\n",
            "Epoch 38/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.5878\n",
            "Epoch 39/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.5396\n",
            "Epoch 40/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.6041\n",
            "Epoch 41/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.5584\n",
            "Epoch 42/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.5205\n",
            "Epoch 43/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.5595\n",
            "Epoch 44/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4885\n",
            "Epoch 45/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4487\n",
            "Epoch 46/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4826\n",
            "Epoch 47/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4722\n",
            "Epoch 48/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4536\n",
            "Epoch 49/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4583\n",
            "Epoch 50/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4318\n",
            "Epoch 51/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4527\n",
            "Epoch 52/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4186\n",
            "Epoch 53/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4497\n",
            "Epoch 54/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4271\n",
            "Epoch 55/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4249\n",
            "Epoch 56/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4014\n",
            "Epoch 57/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.4013\n",
            "Epoch 58/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.3444\n",
            "Epoch 59/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.3926\n",
            "Epoch 60/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.3708\n",
            "Epoch 61/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.3870\n",
            "Epoch 62/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.3298\n",
            "Epoch 63/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.3630\n",
            "Epoch 64/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.3879\n",
            "Epoch 65/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.3708\n",
            "Epoch 66/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.3478\n",
            "Epoch 67/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.3761\n",
            "Epoch 68/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.3484\n",
            "Epoch 69/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.3362\n",
            "Epoch 70/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.3503\n",
            "Epoch 71/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.3292\n",
            "Epoch 72/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.3297\n",
            "Epoch 73/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.3087\n",
            "Epoch 74/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.3077\n",
            "Epoch 75/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2894\n",
            "Epoch 76/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2759\n",
            "Epoch 77/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.3286\n",
            "Epoch 78/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.3340\n",
            "Epoch 79/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.3164\n",
            "Epoch 80/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2918\n",
            "Epoch 81/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2953\n",
            "Epoch 82/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2622\n",
            "Epoch 83/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2714\n",
            "Epoch 84/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2614\n",
            "Epoch 85/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2542\n",
            "Epoch 86/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2646\n",
            "Epoch 87/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2735\n",
            "Epoch 88/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2454\n",
            "Epoch 89/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2208\n",
            "Epoch 90/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2730\n",
            "Epoch 91/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2829\n",
            "Epoch 92/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2662\n",
            "Epoch 93/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2651\n",
            "Epoch 94/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2143\n",
            "Epoch 95/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2367\n",
            "Epoch 96/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2286\n",
            "Epoch 97/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2546\n",
            "Epoch 98/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2352\n",
            "Epoch 99/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2450\n",
            "Epoch 100/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2103\n",
            "Epoch 101/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2204\n",
            "Epoch 102/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2025\n",
            "Epoch 103/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2302\n",
            "Epoch 104/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2232\n",
            "Epoch 105/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2086\n",
            "Epoch 106/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1781\n",
            "Epoch 107/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1886\n",
            "Epoch 108/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2292\n",
            "Epoch 109/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2191\n",
            "Epoch 110/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2351\n",
            "Epoch 111/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1827\n",
            "Epoch 112/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1377\n",
            "Epoch 113/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1590\n",
            "Epoch 114/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1930\n",
            "Epoch 115/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1743\n",
            "Epoch 116/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2021\n",
            "Epoch 117/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2156\n",
            "Epoch 118/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2148\n",
            "Epoch 119/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1662\n",
            "Epoch 120/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1977\n",
            "Epoch 121/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1779\n",
            "Epoch 122/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1890\n",
            "Epoch 123/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2049\n",
            "Epoch 124/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1975\n",
            "Epoch 125/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1895\n",
            "Epoch 126/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2143\n",
            "Epoch 127/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1502\n",
            "Epoch 128/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1366\n",
            "Epoch 129/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1673\n",
            "Epoch 130/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1706\n",
            "Epoch 131/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1793\n",
            "Epoch 132/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1624\n",
            "Epoch 133/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1811\n",
            "Epoch 134/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1629\n",
            "Epoch 135/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1783\n",
            "Epoch 136/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1575\n",
            "Epoch 137/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1500\n",
            "Epoch 138/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1776\n",
            "Epoch 139/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1827\n",
            "Epoch 140/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1460\n",
            "Epoch 141/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1580\n",
            "Epoch 142/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1649\n",
            "Epoch 143/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1553\n",
            "Epoch 144/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1718\n",
            "Epoch 145/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1121\n",
            "Epoch 146/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.2028\n",
            "Epoch 147/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1636\n",
            "Epoch 148/300\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.1699\n",
            "Epoch 149/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1338\n",
            "Epoch 150/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1268\n",
            "Epoch 151/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1484\n",
            "Epoch 152/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1375\n",
            "Epoch 153/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0969\n",
            "Epoch 154/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1662\n",
            "Epoch 155/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1430\n",
            "Epoch 156/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1562\n",
            "Epoch 157/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1446\n",
            "Epoch 158/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1433\n",
            "Epoch 159/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1777\n",
            "Epoch 160/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1443\n",
            "Epoch 161/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1213\n",
            "Epoch 162/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1639\n",
            "Epoch 163/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1284\n",
            "Epoch 164/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1269\n",
            "Epoch 165/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1553\n",
            "Epoch 166/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1435\n",
            "Epoch 167/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1426\n",
            "Epoch 168/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1482\n",
            "Epoch 169/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1353\n",
            "Epoch 170/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1341\n",
            "Epoch 171/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1190\n",
            "Epoch 172/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1233\n",
            "Epoch 173/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1145\n",
            "Epoch 174/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1434\n",
            "Epoch 175/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1464\n",
            "Epoch 176/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1221\n",
            "Epoch 177/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1380\n",
            "Epoch 178/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1312\n",
            "Epoch 179/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1313\n",
            "Epoch 180/300\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.1079\n",
            "Epoch 181/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1138\n",
            "Epoch 182/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1453\n",
            "Epoch 183/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1520\n",
            "Epoch 184/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1477\n",
            "Epoch 185/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1111\n",
            "Epoch 186/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1303\n",
            "Epoch 187/300\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.1363\n",
            "Epoch 188/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1253\n",
            "Epoch 189/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1372\n",
            "Epoch 190/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1345\n",
            "Epoch 191/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0915\n",
            "Epoch 192/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1490\n",
            "Epoch 193/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0988\n",
            "Epoch 194/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1612\n",
            "Epoch 195/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1204\n",
            "Epoch 196/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1052\n",
            "Epoch 197/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1257\n",
            "Epoch 198/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1144\n",
            "Epoch 199/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0863\n",
            "Epoch 200/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0927\n",
            "Epoch 201/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1340\n",
            "Epoch 202/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1064\n",
            "Epoch 203/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1261\n",
            "Epoch 204/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0910\n",
            "Epoch 205/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1312\n",
            "Epoch 206/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0982\n",
            "Epoch 207/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1219\n",
            "Epoch 208/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1219\n",
            "Epoch 209/300\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.1220\n",
            "Epoch 210/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1256\n",
            "Epoch 211/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1381\n",
            "Epoch 212/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1341\n",
            "Epoch 213/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0792\n",
            "Epoch 214/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1204\n",
            "Epoch 215/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1253\n",
            "Epoch 216/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1136\n",
            "Epoch 217/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0899\n",
            "Epoch 218/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0792\n",
            "Epoch 219/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0975\n",
            "Epoch 220/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0739\n",
            "Epoch 221/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1230\n",
            "Epoch 222/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1111\n",
            "Epoch 223/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0907\n",
            "Epoch 224/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0992\n",
            "Epoch 225/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0820\n",
            "Epoch 226/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0908\n",
            "Epoch 227/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0850\n",
            "Epoch 228/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1189\n",
            "Epoch 229/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0845\n",
            "Epoch 230/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0900\n",
            "Epoch 231/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1178\n",
            "Epoch 232/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1039\n",
            "Epoch 233/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0765\n",
            "Epoch 234/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1116\n",
            "Epoch 235/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1280\n",
            "Epoch 236/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0960\n",
            "Epoch 237/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0740\n",
            "Epoch 238/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0738\n",
            "Epoch 239/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1420\n",
            "Epoch 240/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1003\n",
            "Epoch 241/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1185\n",
            "Epoch 242/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0723\n",
            "Epoch 243/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0811\n",
            "Epoch 244/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0888\n",
            "Epoch 245/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1166\n",
            "Epoch 246/300\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0882\n",
            "Epoch 247/300\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.1041\n",
            "Epoch 248/300\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.1349\n",
            "Epoch 249/300\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.1052\n",
            "Epoch 250/300\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0990\n",
            "Epoch 251/300\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.1001\n",
            "Epoch 252/300\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0764\n",
            "Epoch 253/300\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.1034\n",
            "Epoch 254/300\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0878\n",
            "Epoch 255/300\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0680\n",
            "Epoch 256/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0560\n",
            "Epoch 257/300\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0478\n",
            "Epoch 258/300\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0921\n",
            "Epoch 259/300\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0996\n",
            "Epoch 260/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0531\n",
            "Epoch 261/300\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0788\n",
            "Epoch 262/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0969\n",
            "Epoch 263/300\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.1130\n",
            "Epoch 264/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0857\n",
            "Epoch 265/300\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0507\n",
            "Epoch 266/300\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0638\n",
            "Epoch 267/300\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0756\n",
            "Epoch 268/300\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0628\n",
            "Epoch 269/300\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0490\n",
            "Epoch 270/300\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0622\n",
            "Epoch 271/300\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0730\n",
            "Epoch 272/300\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0809\n",
            "Epoch 273/300\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0555\n",
            "Epoch 274/300\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0780\n",
            "Epoch 275/300\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0479\n",
            "Epoch 276/300\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0659\n",
            "Epoch 277/300\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0909\n",
            "Epoch 278/300\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.0612\n",
            "Epoch 279/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0891\n",
            "Epoch 280/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0848\n",
            "Epoch 281/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0475\n",
            "Epoch 282/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0550\n",
            "Epoch 283/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0987\n",
            "Epoch 284/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0517\n",
            "Epoch 285/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0545\n",
            "Epoch 286/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0525\n",
            "Epoch 287/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0696\n",
            "Epoch 288/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0605\n",
            "Epoch 289/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0652\n",
            "Epoch 290/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0477\n",
            "Epoch 291/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1036\n",
            "Epoch 292/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0505\n",
            "Epoch 293/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0628\n",
            "Epoch 294/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0546\n",
            "Epoch 295/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0591\n",
            "Epoch 296/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0791\n",
            "Epoch 297/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0577\n",
            "Epoch 298/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0731\n",
            "Epoch 299/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.1058\n",
            "Epoch 300/300\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "prediction = estimator.predict(X_test)\n",
        "print(\"r_square score: \", r2_score(y_test,prediction))\n",
        "#y_test = sc2.inverse_transform(y_test.reshape(-1,1))\n",
        "yy = np.array(y_test)\n",
        "#yy = np.power(yy, 10)/10\n",
        "#prediction = sc2.inverse_transform(prediction.reshape(-1,1))\n",
        "predd = np.array(prediction)\n",
        "#predd = np.power(predd, 10)/10\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "print(\"RMSE:\")\n",
        "sqrt(mean_squared_error(yy, predd))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIc68VmrYsto",
        "outputId": "c1d8b9c7-a441-408d-8624-f9c5c706c379"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 2ms/step\n",
            "r_square score:  0.9201905364853806\n",
            "RMSE:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9691628488939736"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yy_temp = []\n",
        "predd_temp = []\n",
        "\n",
        "for i in range(len(yy)):\n",
        "  if(yy[i]<10):\n",
        "    yy_temp.append(yy[i])\n",
        "    predd_temp.append(predd[i])\n",
        "\n",
        "plt.scatter(yy_temp, predd_temp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "CojAurDyY1dF",
        "outputId": "2674dc30-dbd6-4470-ac64-807c619e61b4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f8d9bd61210>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled = df_reg2.sample(frac=1).reset_index()\n",
        "\n",
        "columns_dont_want = ['y', 'index', 'inst.name','type', 'CPLEXStatus']\n",
        "select = [x for x in shuffled.columns if x not in columns_dont_want]\n",
        "\n",
        "X = shuffled.loc[:, select]\n",
        "\n",
        "y = shuffled.loc[:,'y']\n",
        "X_modified = X"
      ],
      "metadata": {
        "id": "3UEPEdxNgQ8k"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "oversample = RandomOverSampler(random_state=0)\n",
        "X_modified, y = oversample.fit_resample(X_modified, y.astype('int'))\n",
        "print(X_modified.shape[0])\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote_on_3 = 50\n",
        "#oversample = SMOTE(sampling_strategy={99999:50000})\n",
        "#X_modified, y = oversample.fit_resample(X_modified, y)\n",
        "print(X_modified.shape[0])\n",
        "\n",
        "tmp = X_modified\n",
        "tmp['y'] = y\n",
        "print(tmp.head(60))\n",
        "tmp = tmp.sample(frac = 1).reset_index()\n",
        "#y = np.log10(tmp['y']*10)+20\n",
        "tmp = tmp.dropna()\n",
        "y = tmp['y']\n",
        "X_modified = tmp.drop(['y'], axis = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWjG8zhvgS57",
        "outputId": "76bcb01a-fc90-4434-efc1-f8a93841fe5f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "137994\n",
            "137994\n",
            "        n      k   av.length     std.dev      m     y\n",
            "0   160.0  141.0  661.843750  164.587997   40.0  4510\n",
            "1   160.0  133.0  652.887512  156.741806   80.0  4510\n",
            "2   180.0   47.0   74.566666   14.512603   20.0  4510\n",
            "3   120.0   93.0   99.625000   48.803493   60.0  4510\n",
            "4    80.0   68.0  200.149994   63.368740    8.0  2080\n",
            "5   198.0  173.0  507.494965  164.243591   72.0  4510\n",
            "6   140.0   98.0  110.692856   49.170376   70.0  4510\n",
            "7   198.0  166.0  490.202026  171.781097   22.0  4510\n",
            "8   200.0  179.0  821.250000  195.515594   50.0  4510\n",
            "9   126.0  107.0  501.873016  128.004990   21.0  4510\n",
            "10  180.0  160.0  720.972229  179.274765   72.0   869\n",
            "11  180.0  154.0  706.877808  175.294418   90.0  4510\n",
            "12  198.0  170.0  780.969727  192.608124   66.0  4510\n",
            "13  176.0  151.0  445.829559  143.072861   16.0  4510\n",
            "14  160.0   70.0   98.637497   19.488903   16.0  4510\n",
            "15  220.0   76.0   98.354546   19.407944   20.0  4510\n",
            "16  198.0   50.0   75.121216   14.825057   18.0  4510\n",
            "17  180.0  155.0  718.166687  176.978607   40.0  4510\n",
            "18  120.0  107.0  301.091675  102.802505   24.0  4510\n",
            "19  198.0  169.0  503.666656  171.317017   72.0  3670\n",
            "20  200.0   90.0   50.880001   30.470276   20.0   715\n",
            "21  154.0  133.0  372.785706  132.622253   56.0   751\n",
            "22   90.0   78.0  224.766663   70.754532   10.0  4510\n",
            "23  132.0   68.0  101.159088   20.604170   12.0  4510\n",
            "24  198.0  166.0  512.353516  177.453003   18.0  4510\n",
            "25  180.0  112.0  101.466667   50.076527   90.0  4510\n",
            "26  198.0  176.0  488.575745  165.563660   88.0  1098\n",
            "27   66.0   50.0  101.242424   20.776510    6.0  1045\n",
            "28  198.0  164.0  508.398987  175.845490   66.0   939\n",
            "29  140.0   98.0  110.692856   49.170376   70.0  4510\n",
            "30   72.0   62.0  290.444458   73.034416   12.0  3472\n",
            "31  200.0  119.0  101.699997   49.112724  100.0  4510\n",
            "32  198.0   47.0   76.348488   14.598339   18.0  4510\n",
            "33  126.0  107.0  501.873016  128.004990   28.0  4510\n",
            "34  220.0   88.0   52.309090   28.644171   20.0  1341\n",
            "35  120.0  101.0  467.700012  115.940849   12.0  4510\n",
            "36  200.0  165.0  796.169983  195.300293   20.0  4510\n",
            "37  126.0  107.0  499.809509  128.701874   14.0  4510\n",
            "38  180.0   50.0   74.883331   14.039561   20.0  4510\n",
            "39  180.0   71.0   98.894447   19.570101   20.0  1547\n",
            "40  120.0   93.0   99.625000   48.803493   60.0  4510\n",
            "41   80.0   71.0  334.174988   84.801155    8.0  4510\n",
            "42   60.0   53.0  238.516663   63.221569    6.0  4510\n",
            "43   80.0   57.0   99.199997   47.029049   40.0  4510\n",
            "44  154.0  133.0  372.785706  132.622253   14.0  4510\n",
            "45  220.0  190.0  543.336365  186.884720   80.0  4510\n",
            "46   88.0   72.0  350.647736   87.463364    8.0  4510\n",
            "47  120.0  109.0  472.016663  129.934677   12.0  4510\n",
            "48  144.0  115.0  580.819458  145.891876   24.0  4510\n",
            "49  200.0   73.0   59.285000   22.394224   20.0  4510\n",
            "50  180.0   82.0   49.311111   29.608297   18.0  1548\n",
            "51  198.0  124.0   95.681816   46.024551   33.0  1926\n",
            "52  108.0   93.0  288.018524   83.942337   24.0  3727\n",
            "53  198.0  176.0  488.575745  165.563660   22.0  4510\n",
            "54   80.0   71.0  197.987503   69.608765    8.0  4510\n",
            "55  126.0   96.0  114.595238   48.629894   21.0  1834\n",
            "56  120.0   62.0  103.591667   20.267330   12.0  4510\n",
            "57  180.0  153.0  739.488892  194.321213   18.0  4510\n",
            "58  120.0   63.0   57.441666   23.780083   12.0  1035\n",
            "59  120.0   44.0   74.566666   14.505133   12.0  3650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense ,Dropout,BatchNormalization\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from keras.layers import Activation\n",
        "from keras import backend as K\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras \n",
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "\n",
        "def custom_activation(x):\n",
        "    #return K.relu(tensorflow.subtract(x,-1)) - K.relu(tensorflow.subtract(x,3.5))\n",
        "    return 4510*1/(1+K.exp(-x))\n",
        "\n",
        "\n",
        "get_custom_objects().update({'custom_activation': Activation(custom_activation)})\n",
        "\n",
        "def make_model():\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Dense(16, input_dim=X_modified.shape[1], activation='relu'))\n",
        "    #model.add(Dropout(0.25))\n",
        "    \n",
        "    model.add(Dense(32, activation = 'relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "\n",
        "    model.add(Dense(16, activation = 'relu'))\n",
        "\n",
        "    model.add(Dense(4, activation = 'relu'))\n",
        "\n",
        "    model.add(Dense(1, activation = custom_activation))\n",
        "    model.compile(loss='mean_squared_error', optimizer='Adam')\n",
        "    return model"
      ],
      "metadata": {
        "id": "Luwo5AkigdEG"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc = StandardScaler()\n",
        "sc2 = StandardScaler()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_modified, y, random_state=0, train_size = 0.83)\n",
        "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=0, train_size = 0.83)\n",
        "\n",
        "\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test_copy = X_test\n",
        "X_test = sc.transform(X_test)\n",
        "#X_val = sc.transform(X_val)\n",
        "\n",
        "#y_train = sc2.fit_transform(y_train.values.reshape(-1,1))\n",
        "#y_test = sc2.transform(y_test.values.reshape(-1,1))\n",
        "\n",
        "estimator = KerasRegressor(build_fn=make_model, epochs=60, batch_size=128, verbose=1)\n",
        "\n",
        "#history=estimator.fit(X_train,y_train)\n",
        "from keras.metrics import accuracy\n",
        "from keras import callbacks\n",
        "earlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", \n",
        "                                        mode =\"min\", patience = 5, \n",
        "                                        restore_best_weights = True)\n",
        "  \n",
        "history = estimator.fit(X_train, y_train, shuffle = True)\n",
        "#plt.plot(history.history['val_accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apyKMbpJgne2",
        "outputId": "c6fd2508-27a3-4565-a7ee-c69228a2c248"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "895/895 [==============================] - 3s 2ms/step - loss: 171691.3906\n",
            "Epoch 2/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 69124.2500\n",
            "Epoch 3/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 50453.1406\n",
            "Epoch 4/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 42589.9766\n",
            "Epoch 5/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 35827.5078\n",
            "Epoch 6/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 30069.3750\n",
            "Epoch 7/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 25128.5000\n",
            "Epoch 8/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 23036.4023\n",
            "Epoch 9/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 21505.5000\n",
            "Epoch 10/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 20729.3086\n",
            "Epoch 11/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 19235.7578\n",
            "Epoch 12/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 17253.3066\n",
            "Epoch 13/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 19548.8730\n",
            "Epoch 14/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 18426.9551\n",
            "Epoch 15/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 17749.9492\n",
            "Epoch 16/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 17013.5801\n",
            "Epoch 17/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 16889.1465\n",
            "Epoch 18/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 15165.2119\n",
            "Epoch 19/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 17141.6016\n",
            "Epoch 20/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 14595.3418\n",
            "Epoch 21/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 15183.2969\n",
            "Epoch 22/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 14904.9619\n",
            "Epoch 23/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 14951.2910\n",
            "Epoch 24/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 15390.1182\n",
            "Epoch 25/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 13839.2588\n",
            "Epoch 26/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 13168.4141\n",
            "Epoch 27/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 13031.1699\n",
            "Epoch 28/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 13907.9873\n",
            "Epoch 29/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 13099.8008\n",
            "Epoch 30/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 13320.5127\n",
            "Epoch 31/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 14051.0762\n",
            "Epoch 32/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 11999.4766\n",
            "Epoch 33/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 11941.0908\n",
            "Epoch 34/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 12191.0117\n",
            "Epoch 35/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 12041.5352\n",
            "Epoch 36/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 11943.0508\n",
            "Epoch 37/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 12170.9590\n",
            "Epoch 38/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 12739.4141\n",
            "Epoch 39/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 13419.6387\n",
            "Epoch 40/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 12658.6484\n",
            "Epoch 41/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 11462.5615\n",
            "Epoch 42/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 11158.5137\n",
            "Epoch 43/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 11357.5732\n",
            "Epoch 44/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 11711.8486\n",
            "Epoch 45/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 11980.4297\n",
            "Epoch 46/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 12077.9375\n",
            "Epoch 47/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 11005.4922\n",
            "Epoch 48/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 11500.5117\n",
            "Epoch 49/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 11211.2725\n",
            "Epoch 50/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 11041.1904\n",
            "Epoch 51/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 11273.9727\n",
            "Epoch 52/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 11728.8809\n",
            "Epoch 53/60\n",
            "895/895 [==============================] - 3s 3ms/step - loss: 10263.5430\n",
            "Epoch 54/60\n",
            "895/895 [==============================] - 3s 3ms/step - loss: 10935.4277\n",
            "Epoch 55/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 10804.4434\n",
            "Epoch 56/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 11027.2344\n",
            "Epoch 57/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 9319.8047\n",
            "Epoch 58/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 10670.0596\n",
            "Epoch 59/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 9863.9600\n",
            "Epoch 60/60\n",
            "895/895 [==============================] - 2s 2ms/step - loss: 9615.3262\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "prediction = estimator.predict(X_test)\n",
        "print(\"r_square score: \", r2_score(y_test,prediction))\n",
        "#y_test = sc2.inverse_transform(y_test.reshape(-1,1))\n",
        "yy = np.array(y_test)\n",
        "#yy = np.power(yy, 10)/10\n",
        "#prediction = sc2.inverse_transform(prediction.reshape(-1,1))\n",
        "predd = np.array(prediction)\n",
        "#predd = np.power(predd, 10)/10\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "print(\"RMSE:\")\n",
        "sqrt(mean_squared_error(yy, predd))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-D2S_qagoEL",
        "outputId": "bba07b2a-05db-43fd-ed6f-d622a14a3d95"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "184/184 [==============================] - 0s 1ms/step\n",
            "r_square score:  0.9873045742612685\n",
            "RMSE:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "132.39039982408522"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yy_temp = []\n",
        "predd_temp = []\n",
        "\n",
        "for i in range(len(yy)):\n",
        "  if(yy[i]>1000):\n",
        "    yy_temp.append(yy[i])\n",
        "    predd_temp.append(predd[i])\n",
        "\n",
        "plt.scatter(yy_temp, predd_temp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "2ezns0RWgtkq",
        "outputId": "0c9ebac0-c184-4046-fe31-e0a52f480b50"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f8d9bbeda10>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}