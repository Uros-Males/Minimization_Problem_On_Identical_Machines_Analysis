{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_Network_Regression.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOlnWBpXnmHEfhfbExMyDn6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uros-Males/Minimization_Problem_On_Identical_Machines_Analysis/blob/main/Neural_Network_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YiQcuNkDxHUf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "defcb5b3-f1ff-4c9a-d6f4-166ccda66127"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8750\n"
          ]
        }
      ],
      "source": [
        "#IN PROGRESS....\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "import time\n",
        "\n",
        "df = pd.read_csv('/content/sample_data/C-instances-runtime-analysis.csv')\n",
        "print(df.shape[0])\n",
        "\n",
        "shuffled = df.sample(frac=1).reset_index()\n",
        "\n",
        "columns_dont_want = ['y', 'index', 'inst.name','type']\n",
        "select = [x for x in shuffled.columns if x not in columns_dont_want]\n",
        "\n",
        "X = shuffled.loc[:, select]\n",
        "\n",
        "y = shuffled.loc[:,'y']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#X_modified = X\n",
        "X_modified = X.drop(['max', 'n/m', '(n/m)^2', '(n/m)^3', 'm/n', '(m/n)^2', '(m/n)^3', 'class', 'av.length'], axis = 1)"
      ],
      "metadata": {
        "id": "Jzlk_oaRyQjm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense ,Dropout,BatchNormalization\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from keras.layers import Activation\n",
        "from keras import backend as K\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "\n",
        "import tensorflow\n",
        "from tensorflow import keras \n",
        "\n",
        "\n",
        "def custom_activation(x):\n",
        "    return 99999*1/(1+K.exp(-x))\n",
        "\n",
        "get_custom_objects().update({'custom_activation': Activation(custom_activation)})\n",
        "\n",
        "def make_model():\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Dense(8, input_dim=X_modified.shape[1], activation='relu'))\n",
        "    #model.add(Dropout(0.25))\n",
        "    \n",
        "    \n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    #model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Dense(8, activation='relu'))\n",
        "    #model.add(Dropout(0.15))\n",
        "    \n",
        "    model.add(Dense(4, activation='relu'))\n",
        "    \n",
        "    model.add(Dense(1, activation = custom_activation))\n",
        "    #model.add(Activation(custom_activation, name='SpecialActivation'))\n",
        "    model.compile(loss='mean_squared_error', optimizer=tensorflow.keras.optimizers.Adam(clipnorm=1))\n",
        "    return model"
      ],
      "metadata": {
        "id": "7_m2qFDRya94"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler =  MinMaxScaler()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_modified, y, random_state=0, train_size = 0.83)\n",
        "\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.fit_transform(X_test)\n",
        "\n",
        "estimator = KerasRegressor(build_fn=make_model, epochs=300, batch_size=32, verbose=1)\n",
        "\n",
        "#history=estimator.fit(X_train,y_train)\n",
        "from keras.metrics import accuracy\n",
        "from keras import callbacks\n",
        "earlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", \n",
        "                                        mode =\"min\", patience = 25, \n",
        "                                        restore_best_weights = True)\n",
        "  \n",
        "history = estimator.fit(X_train, y_train, validation_data =(X_test, y_test), \n",
        "                    callbacks =[earlystopping])\n",
        "#plt.plot(history.history['val_accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JHin8gI5ftQ",
        "outputId": "f49760e3-5ec1-4034-b7ea-84fa8900ef4e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "227/227 [==============================] - 3s 7ms/step - loss: 1038734080.0000 - val_loss: 7160971.0000\n",
            "Epoch 2/300\n",
            "227/227 [==============================] - 1s 6ms/step - loss: 775849.6875 - val_loss: 652309.5625\n",
            "Epoch 3/300\n",
            "227/227 [==============================] - 1s 5ms/step - loss: 416030.6562 - val_loss: 422107.5625\n",
            "Epoch 4/300\n",
            "227/227 [==============================] - 1s 5ms/step - loss: 431850.1875 - val_loss: 280497.5000\n",
            "Epoch 5/300\n",
            "227/227 [==============================] - 1s 5ms/step - loss: 359623.1875 - val_loss: 274917.4062\n",
            "Epoch 6/300\n",
            "227/227 [==============================] - 1s 5ms/step - loss: 317178.3750 - val_loss: 227185.7188\n",
            "Epoch 7/300\n",
            "227/227 [==============================] - 1s 5ms/step - loss: 246657.3438 - val_loss: 197076.2656\n",
            "Epoch 8/300\n",
            "227/227 [==============================] - 1s 4ms/step - loss: 273779.0000 - val_loss: 330061.2812\n",
            "Epoch 9/300\n",
            "227/227 [==============================] - 1s 5ms/step - loss: 227409.0469 - val_loss: 201243.3125\n",
            "Epoch 10/300\n",
            "227/227 [==============================] - 1s 5ms/step - loss: 223477.5625 - val_loss: 288721.6250\n",
            "Epoch 11/300\n",
            "227/227 [==============================] - 1s 5ms/step - loss: 218697.6719 - val_loss: 186963.7812\n",
            "Epoch 12/300\n",
            "227/227 [==============================] - 1s 5ms/step - loss: 180225.1250 - val_loss: 184769.7344\n",
            "Epoch 13/300\n",
            "227/227 [==============================] - 1s 5ms/step - loss: 175161.8906 - val_loss: 216624.5156\n",
            "Epoch 14/300\n",
            "227/227 [==============================] - 2s 9ms/step - loss: 172533.3125 - val_loss: 169169.1250\n",
            "Epoch 15/300\n",
            "227/227 [==============================] - 1s 6ms/step - loss: 165170.4688 - val_loss: 216601.4688\n",
            "Epoch 16/300\n",
            "227/227 [==============================] - 1s 5ms/step - loss: 163129.4062 - val_loss: 182544.1719\n",
            "Epoch 17/300\n",
            "227/227 [==============================] - 1s 5ms/step - loss: 152056.4375 - val_loss: 205366.3438\n",
            "Epoch 18/300\n",
            "227/227 [==============================] - 1s 5ms/step - loss: 159529.5781 - val_loss: 181394.4375\n",
            "Epoch 19/300\n",
            "227/227 [==============================] - 1s 5ms/step - loss: 160208.6719 - val_loss: 198442.0625\n",
            "Epoch 20/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 151806.1719 - val_loss: 176459.5156\n",
            "Epoch 21/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 151276.0156 - val_loss: 169585.7031\n",
            "Epoch 22/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 153574.0781 - val_loss: 188443.3594\n",
            "Epoch 23/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 157432.4375 - val_loss: 167678.8438\n",
            "Epoch 24/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 155352.2812 - val_loss: 188276.2969\n",
            "Epoch 25/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 159047.6562 - val_loss: 195758.0625\n",
            "Epoch 26/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 152659.0938 - val_loss: 164391.1562\n",
            "Epoch 27/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 153991.5156 - val_loss: 160637.9062\n",
            "Epoch 28/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 154466.8281 - val_loss: 269274.7188\n",
            "Epoch 29/300\n",
            "227/227 [==============================] - 1s 3ms/step - loss: 154547.4688 - val_loss: 207453.4688\n",
            "Epoch 30/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 160576.0938 - val_loss: 160660.9688\n",
            "Epoch 31/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 152270.4844 - val_loss: 160961.6406\n",
            "Epoch 32/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 149125.9219 - val_loss: 160813.3594\n",
            "Epoch 33/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 147323.7344 - val_loss: 191842.5625\n",
            "Epoch 34/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 149331.4844 - val_loss: 159153.1250\n",
            "Epoch 35/300\n",
            "227/227 [==============================] - 1s 3ms/step - loss: 149957.0469 - val_loss: 158978.1562\n",
            "Epoch 36/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 149533.4219 - val_loss: 180459.0781\n",
            "Epoch 37/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 150100.7500 - val_loss: 163778.2031\n",
            "Epoch 38/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 147386.7812 - val_loss: 161594.8125\n",
            "Epoch 39/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 150354.4844 - val_loss: 157107.2344\n",
            "Epoch 40/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 149965.7188 - val_loss: 173143.5312\n",
            "Epoch 41/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 147765.5938 - val_loss: 166958.5781\n",
            "Epoch 42/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 139240.6406 - val_loss: 197188.7344\n",
            "Epoch 43/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 143816.4844 - val_loss: 157826.6406\n",
            "Epoch 44/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 143541.7656 - val_loss: 173710.1562\n",
            "Epoch 45/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 147347.3125 - val_loss: 174521.2969\n",
            "Epoch 46/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 145363.0781 - val_loss: 156312.8750\n",
            "Epoch 47/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 137734.7812 - val_loss: 160914.3594\n",
            "Epoch 48/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 135658.9062 - val_loss: 158164.5000\n",
            "Epoch 49/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 145980.0312 - val_loss: 163572.0938\n",
            "Epoch 50/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 136686.7656 - val_loss: 155604.4375\n",
            "Epoch 51/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 137684.8281 - val_loss: 172640.2969\n",
            "Epoch 52/300\n",
            "227/227 [==============================] - 0s 2ms/step - loss: 145222.4531 - val_loss: 172334.8438\n",
            "Epoch 53/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 138258.1562 - val_loss: 194988.2344\n",
            "Epoch 54/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 139642.6719 - val_loss: 288684.4062\n",
            "Epoch 55/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 141758.6875 - val_loss: 164476.4531\n",
            "Epoch 56/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 135793.5781 - val_loss: 171028.7500\n",
            "Epoch 57/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 133569.8125 - val_loss: 158389.1094\n",
            "Epoch 58/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 136545.6562 - val_loss: 154044.5625\n",
            "Epoch 59/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 138030.5781 - val_loss: 239526.5312\n",
            "Epoch 60/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 136997.5156 - val_loss: 180867.5938\n",
            "Epoch 61/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 133229.1250 - val_loss: 159898.4531\n",
            "Epoch 62/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 135596.6406 - val_loss: 160278.2188\n",
            "Epoch 63/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 132736.9531 - val_loss: 162618.7344\n",
            "Epoch 64/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 135127.8594 - val_loss: 268010.6562\n",
            "Epoch 65/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 144065.3594 - val_loss: 220208.5781\n",
            "Epoch 66/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 135870.2344 - val_loss: 168206.1094\n",
            "Epoch 67/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 134587.2812 - val_loss: 184976.8594\n",
            "Epoch 68/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 139004.7188 - val_loss: 191129.3281\n",
            "Epoch 69/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 124245.7734 - val_loss: 173492.0469\n",
            "Epoch 70/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 135808.5625 - val_loss: 156689.6562\n",
            "Epoch 71/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 132782.7812 - val_loss: 162564.8125\n",
            "Epoch 72/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 130252.9766 - val_loss: 151473.2188\n",
            "Epoch 73/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 131480.7656 - val_loss: 154764.7656\n",
            "Epoch 74/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 138302.1406 - val_loss: 159003.8281\n",
            "Epoch 75/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 139195.5312 - val_loss: 150986.6719\n",
            "Epoch 76/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 140181.7969 - val_loss: 160071.9531\n",
            "Epoch 77/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 132981.6875 - val_loss: 158389.9219\n",
            "Epoch 78/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 129311.1094 - val_loss: 168693.0156\n",
            "Epoch 79/300\n",
            "227/227 [==============================] - 1s 3ms/step - loss: 135481.6406 - val_loss: 305587.6250\n",
            "Epoch 80/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 148584.2812 - val_loss: 151294.6875\n",
            "Epoch 81/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 136497.5312 - val_loss: 158144.0938\n",
            "Epoch 82/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 128762.6484 - val_loss: 166042.5312\n",
            "Epoch 83/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 136518.4375 - val_loss: 173932.7969\n",
            "Epoch 84/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 130509.3203 - val_loss: 174821.2188\n",
            "Epoch 85/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 131841.2188 - val_loss: 178837.3438\n",
            "Epoch 86/300\n",
            "227/227 [==============================] - 1s 3ms/step - loss: 131735.6562 - val_loss: 155759.6406\n",
            "Epoch 87/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 128799.6641 - val_loss: 194698.7344\n",
            "Epoch 88/300\n",
            "227/227 [==============================] - 1s 3ms/step - loss: 132060.0469 - val_loss: 183811.3750\n",
            "Epoch 89/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 134022.5156 - val_loss: 191049.9531\n",
            "Epoch 90/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 126379.9219 - val_loss: 160627.7812\n",
            "Epoch 91/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 130170.4062 - val_loss: 152884.7031\n",
            "Epoch 92/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 129778.4688 - val_loss: 148579.1562\n",
            "Epoch 93/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 131267.2500 - val_loss: 235460.1875\n",
            "Epoch 94/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 131980.3281 - val_loss: 186483.1875\n",
            "Epoch 95/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 134394.8750 - val_loss: 175982.6094\n",
            "Epoch 96/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 130585.5234 - val_loss: 152669.1094\n",
            "Epoch 97/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 129805.7578 - val_loss: 152484.9062\n",
            "Epoch 98/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 126901.0000 - val_loss: 165953.0625\n",
            "Epoch 99/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 128765.8047 - val_loss: 186060.6250\n",
            "Epoch 100/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 130749.8516 - val_loss: 220199.6562\n",
            "Epoch 101/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 142621.5312 - val_loss: 158251.1719\n",
            "Epoch 102/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 127650.3359 - val_loss: 163691.4531\n",
            "Epoch 103/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 128716.1328 - val_loss: 173258.0156\n",
            "Epoch 104/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 131265.4531 - val_loss: 170202.7500\n",
            "Epoch 105/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 131168.2344 - val_loss: 168305.8125\n",
            "Epoch 106/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 134818.5938 - val_loss: 163860.8750\n",
            "Epoch 107/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 129218.5547 - val_loss: 151280.0938\n",
            "Epoch 108/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 129087.7812 - val_loss: 154141.3906\n",
            "Epoch 109/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 126331.3906 - val_loss: 156241.6562\n",
            "Epoch 110/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 128915.8359 - val_loss: 150909.5625\n",
            "Epoch 111/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 125538.5469 - val_loss: 151256.4375\n",
            "Epoch 112/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 129545.2422 - val_loss: 171689.4219\n",
            "Epoch 113/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 127663.9219 - val_loss: 157734.2500\n",
            "Epoch 114/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 127434.6875 - val_loss: 190907.1875\n",
            "Epoch 115/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 129675.5156 - val_loss: 148458.1875\n",
            "Epoch 116/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 127443.3125 - val_loss: 146804.9375\n",
            "Epoch 117/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 127576.6250 - val_loss: 188803.7188\n",
            "Epoch 118/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 124849.4297 - val_loss: 147351.3125\n",
            "Epoch 119/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 125994.6797 - val_loss: 160376.8906\n",
            "Epoch 120/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 127752.4609 - val_loss: 149281.6719\n",
            "Epoch 121/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 126202.4531 - val_loss: 150633.4844\n",
            "Epoch 122/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 127456.5234 - val_loss: 187684.7031\n",
            "Epoch 123/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 126418.0000 - val_loss: 153766.2031\n",
            "Epoch 124/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 129649.0859 - val_loss: 160903.8594\n",
            "Epoch 125/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 128795.9297 - val_loss: 250233.1406\n",
            "Epoch 126/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 128909.2969 - val_loss: 151137.4375\n",
            "Epoch 127/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 124450.7344 - val_loss: 154200.2500\n",
            "Epoch 128/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 131524.9688 - val_loss: 147954.2188\n",
            "Epoch 129/300\n",
            "227/227 [==============================] - 1s 3ms/step - loss: 126535.4062 - val_loss: 151535.4688\n",
            "Epoch 130/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 127198.7734 - val_loss: 155345.0156\n",
            "Epoch 131/300\n",
            "227/227 [==============================] - 1s 3ms/step - loss: 125817.5234 - val_loss: 173319.2500\n",
            "Epoch 132/300\n",
            "227/227 [==============================] - 1s 3ms/step - loss: 127091.9766 - val_loss: 152893.5000\n",
            "Epoch 133/300\n",
            "227/227 [==============================] - 1s 3ms/step - loss: 122948.2500 - val_loss: 159624.1719\n",
            "Epoch 134/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 127091.9453 - val_loss: 157178.3906\n",
            "Epoch 135/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 126228.1016 - val_loss: 211577.4375\n",
            "Epoch 136/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 127219.3594 - val_loss: 160266.6719\n",
            "Epoch 137/300\n",
            "227/227 [==============================] - 1s 3ms/step - loss: 131160.0469 - val_loss: 163908.0625\n",
            "Epoch 138/300\n",
            "227/227 [==============================] - 1s 3ms/step - loss: 128004.0000 - val_loss: 236698.3281\n",
            "Epoch 139/300\n",
            "227/227 [==============================] - 1s 3ms/step - loss: 129657.7422 - val_loss: 158339.8750\n",
            "Epoch 140/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 128558.0000 - val_loss: 172467.5469\n",
            "Epoch 141/300\n",
            "227/227 [==============================] - 1s 2ms/step - loss: 130466.5547 - val_loss: 153403.8125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "jG1I_-jh7fu0",
        "outputId": "d8715a67-b8c0-4143-8148-c45f3c833552"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f10c3630590>]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEDCAYAAAAlRP8qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAUC0lEQVR4nO3df4xl5X3f8ffn3jVEjl1jezcuYbGXpOukW8uO6YriulJRbScLililaSqQo8QtCv+ElCZOK6grmlJVles0adJgJ9uWolgOFNM0Xbmb0BQTuaqCyxIHwo+ss8XYLHHK2sakquXi3f32j3tm5twfy1wvd7j3mb5f0mjuOefZmS9HM595eM733JOqQpLUvsGyC5AkLYaBLknbhIEuSduEgS5J24SBLknbhIEuSdvEUgM9ye1Jnk3y6Bxj35TkviSPJPmdJLtfjholqRXLnqHfARyYc+zPAr9aVW8FbgX+2VYVJUktWmqgV9WngK/09yX5ziS/leShJP8tyXd3h/YBn+xe3w8cfBlLlaSVt+wZ+iyHgJ+oqr8I/DTw4W7/w8Bf717/APDqJK9fQn2StJJ2LLuAviSvAv4y8PEka7vP7z7/NPBLSd4HfAp4Bjj9ctcoSatqpQKd0f8xfLWqvmfyQFX9Md0MvQv+H6yqr77M9UnSylqpJZeq+lPgc0l+CCAjb+te70yyVu/NwO1LKlOSVtKy2xbvBH4X+K4kJ5JcB7wXuC7Jw8BjbFz8vAI4luSzwBuAf7qEkiVpZcW3z5Wk7WGlllwkSeduaRdFd+7cWXv27FnWt5ekJj300ENfqqpds44tLdD37NnD0aNHl/XtJalJST5/tmMuuUjSNmGgS9I2YaBL0jZhoEvSNmGgS9I2YaBL0jZhoEvSNtFcoD/41Ff4F//lGN84fWbZpUjSSmku0H/v88/xrz55nBdOGeiS1NdcoA8HowdfnPZNxSRpTHOBPuieZFRO0CVpTIOBPvrsDF2Sxm0a6EluT/JskkfPcjxJfjHJ8SSPJLl08WVuWF9yOWOgS1LfPDP0O4ADL3L8SmBv93E98JGXXtbZDbpAP+MMXZLGbBroVfUp4CsvMuQg8Ks18gBwQZILF1XgpGGcoUvSLItYQ78IeLq3faLbtyUGLrlI0kwv60XRJNcnOZrk6MmTJ8/pa6x3uZjnkjRmEYH+DHBxb3t3t29KVR2qqv1VtX/XrplPUNrUsKvYLhdJGreIQD8M/EjX7XI58HxVfXEBX3emgWvokjTTps8UTXIncAWwM8kJ4B8BrwCoql8GjgBXAceBrwF/a6uKhY22RbtcJGncpoFeVdducryAH19YRZuwy0WSZmvvTlFn6JI0U3uB3s3Qz/heLpI0prlAt8tFkmZrLtDtcpGk2ZoLdLtcJGm29gLdGbokzdRcoCfO0CVpluYCfX3JxS4XSRrTYKCPPtvlIknjmgv0jT50A12S+poLdB9BJ0mzNRfoAy+KStJMBrokbRPNBfrGksuSC5GkFdNgoI8+2+UiSeOaC3S7XCRptuYC3S4XSZqtuUD3oqgkzdZeoPtui5I0U3OBvvFui0suRJJWTHOBPrDLRZJmai7Qh3a5SNJM7QW6a+iSNFNzgR6fWCRJMzUX6M7QJWm29gLdLhdJmqm5QF/rcnGGLknjmgv0oWvokjRTc4Hurf+SNFt7gT6wD12SZpkr0JMcSHIsyfEkN804/sYk9yf5TJJHkly1+FI3DAfxTlFJmrBpoCcZArcBVwL7gGuT7JsY9g+Bu6vq7cA1wIcXXWjfMLHLRZImzDNDvww4XlVPVtULwF3AwYkxBfyZ7vVrgD9eXInTBgPX0CVp0jyBfhHwdG/7RLev72eAH05yAjgC/MSsL5Tk+iRHkxw9efLkOZQ7MkxcQ5ekCYu6KHotcEdV7QauAj6aZOprV9WhqtpfVft37dp1zt9sENfQJWnSPIH+DHBxb3t3t6/vOuBugKr6XeBbgJ2LKHCWwcAZuiRNmifQHwT2JrkkyXmMLnoenhjzBeBdAEn+PKNAP/c1lU3Y5SJJ0zYN9Ko6BdwA3As8waib5bEktya5uhv2fuDHkjwM3Am8r2rrEndgl4skTdkxz6CqOsLoYmd/3y29148D71xsaWc3HHhjkSRNau5OURjN0G1blKRxzQa6a+iSNK7JQB/a5SJJU5oN9NPmuSSNaTLQB/GiqCRNajLQhwMvikrSpCYDfdSHbqBLUl+zge4MXZLGNRnow4EzdEma1GSgD+xykaQpTQb60C4XSZrSZqDb5SJJU5oM9NjlIklTmgz0oV0ukjSlzUC3y0WSpjQZ6Ha5SNK0JgN9GNjCByJJUpOaDHRv/ZekaW0GumvokjSlyUC3y0WSprUZ6M7QJWlKk4E+GATzXJLGNRnow+CSiyRNaDLQ7XKRpGltBvogvtuiJE1oMtCHCaddcpGkMU0G+qgPfdlVSNJqaTLQhwNv/ZekSU0G+sAlF0maMlegJzmQ5FiS40luOsuYv5nk8SSPJfm1xZY5zi4XSZq2Y7MBSYbAbcB7gBPAg0kOV9XjvTF7gZuBd1bVc0m+basKhu4RdAa6JI2ZZ4Z+GXC8qp6sqheAu4CDE2N+DLitqp4DqKpnF1vmuOHAJRdJmjRPoF8EPN3bPtHt63sz8OYk/z3JA0kOzPpCSa5PcjTJ0ZMnT55bxYyWXM7Y5SJJYxZ1UXQHsBe4ArgW+NdJLpgcVFWHqmp/Ve3ftWvXOX+z4cBb/yVp0jyB/gxwcW97d7ev7wRwuKq+UVWfAz7LKOC3hF0ukjRtnkB/ENib5JIk5wHXAIcnxvwGo9k5SXYyWoJ5coF1jhkkVNmLLkl9mwZ6VZ0CbgDuBZ4A7q6qx5LcmuTqbti9wJeTPA7cD/y9qvryVhU9HATA1kVJ6tm0bRGgqo4ARyb23dJ7XcBPdR9bbj3Qq+b7D5Ck/w80e6coYKeLJPU0Guijz3a6SNKGJgO9v+QiSRppMtA3llwMdEla02Sg2+UiSdOaDPSBSy6SNKXJQB92Sy7muSRtaDLQ17pcXHKRpA1tBrpr6JI0pclAX1tysQ9dkja0GejO0CVpSpOBvrbk4gxdkja0Gejrt/4vtw5JWiVNBvraGrpLLpK0oclAt8tFkqY1Geh2uUjStDYD3Rm6JE1pMtA3ulyWXIgkrZA2A90HXEjSlCYD3S4XSZrWZKCvL7kY6JK0rslA9xF0kjStyUAfuOQiSVMaDfTRZyfokrShyUC3D12SpjUZ6OtLLk7RJWldk4E+tMtFkqY0HejO0CVpQ5OBPoi3/kvSpLkCPcmBJMeSHE9y04uM+8EklWT/4kqctn7rv4kuSes2DfQkQ+A24EpgH3Btkn0zxr0auBH49KKLnGSXiyRNm2eGfhlwvKqerKoXgLuAgzPG/RPgg8DXF1jfTHa5SNK0eQL9IuDp3vaJbt+6JJcCF1fVf36xL5Tk+iRHkxw9efLkN13sGrtcJGnaS74ommQA/Bzw/s3GVtWhqtpfVft37dp1zt/TLhdJmjZPoD8DXNzb3t3tW/Nq4C3A7yR5CrgcOLyVF0az/n7oW/UdJKk98wT6g8DeJJckOQ+4Bji8drCqnq+qnVW1p6r2AA8AV1fV0S2pmN4zRU10SVq3aaBX1SngBuBe4Ang7qp6LMmtSa7e6gJnsctFkqbtmGdQVR0Bjkzsu+UsY6946WW9uI1nihrokrSmyTtFfQSdJE1rM9AH3vovSZOaDPSNLhcTXZLWNBnoLrlI0rQ2A90uF0ma0mSgJyFxyUWS+poMdBgtuzhDl6QNzQb6ILHLRZJ62g30gUsuktTXbKC75CJJ45oN9MHAQJekvmYDfTiISy6S1NNuoMdAl6S+ZgM9CafPLLsKSVodzQb6cOADLiSpr91AT3ymqCT1NBvog0GcoUtST7OBPhw4Q5ekvmYD3Vv/JWlcw4HuRVFJ6ms20IfeKSpJY5oN9IFdLpI0ptlAH9rlIklj2g50Z+iStK7ZQE/CafNcktY1G+hDu1wkaUy7gW6XiySNaTbQ7XKRpHHNBrpdLpI0bq5AT3IgybEkx5PcNOP4TyV5PMkjSe5L8qbFlzpu4AMuJGnMpoGeZAjcBlwJ7AOuTbJvYthngP1V9VbgHuCfL7rQSYOBXS6S1DfPDP0y4HhVPVlVLwB3AQf7A6rq/qr6Wrf5ALB7sWVOs8tFksbNE+gXAU/3tk90+87mOuA3Zx1Icn2So0mOnjx5cv4qZ7DLRZLGLfSiaJIfBvYDH5p1vKoOVdX+qtq/a9eul/S9XEOXpHE75hjzDHBxb3t3t29MkncDHwD+alX938WUd3be+i9J4+aZoT8I7E1ySZLzgGuAw/0BSd4O/ApwdVU9u/gypw3ikosk9W0a6FV1CrgBuBd4Ari7qh5LcmuSq7thHwJeBXw8ye8nOXyWL7cwg4FPLJKkvnmWXKiqI8CRiX239F6/e8F1bWoYnKFLUk+zd4oO7HKRpDHNBvrQLhdJGtNsoNu2KEnj2g30QTh9ZtlVSNLqaDbQhwOcoUtST7uBbh+6JI1pNtAHvh+6JI1pNtCHPrFIksY0G+gD38tFksa0G+gJZ+xykaR1zQb6cIBLLpLU026g2+UiSWOaDfTBIICPoZOkNc0G+jBdoLvsIklAw4G+NkN3HV2SRtoN9LUZup0ukgQ0HOjDrnJn6JI00mygr83Q7XSRpJFmA31ol4skjWk20Ad2uUjSmHYD3S4XSRrTbKAP7XKRpDHtBrpdLpI0ptlA3+hDN9AlCRoO9PUuF2fokgQ0HOj2oUvSuHYD3Rm6JI1pNtCH6zP0JRciSSui3UBf63JxyUWSgDkDPcmBJMeSHE9y04zj5yf5993xTyfZs+hCJ3mnqCSN2zTQkwyB24ArgX3AtUn2TQy7Dniuqv4c8PPABxdd6CQDXZLG7ZhjzGXA8ap6EiDJXcBB4PHemIPAz3Sv7wF+KUmqti5tdwxHgX7NoQd4xXDAjkEYDrIe9N0nuk9kbcf6dn/Mi/+b9HaOfxVJ+ubd+O43c/Xbvn3hX3eeQL8IeLq3fQL4S2cbU1WnkjwPvB74Un9QkuuB6wHe+MY3nmPJI5dd8jpufNde/vfXT3H6zBlOV3HqdFEFxejvyNqfk7W/Khvbtb5z41idZez0MUl6KV77yldsydedJ9AXpqoOAYcA9u/f/5Ly8ZXn7eAn3/PmhdQlSdvBPBdFnwEu7m3v7vbNHJNkB/Aa4MuLKFCSNJ95Av1BYG+SS5KcB1wDHJ4Ycxj40e713wA+uZXr55KkaZsuuXRr4jcA9wJD4PaqeizJrcDRqjoM/Fvgo0mOA19hFPqSpJfRXGvoVXUEODKx75be668DP7TY0iRJ34xm7xSVJI0z0CVpmzDQJWmbMNAlaZvIsroLk5wEPn+O/3wnE3ehrriW6m2pVmir3pZqhbbqbalWeGn1vqmqds06sLRAfymSHK2q/cuuY14t1dtSrdBWvS3VCm3V21KtsHX1uuQiSduEgS5J20SrgX5o2QV8k1qqt6Vaoa16W6oV2qq3pVphi+ptcg1dkjSt1Rm6JGmCgS5J20Rzgb7ZA6uXKcnFSe5P8niSx5Lc2O1/XZLfTvJH3efXLrvWNUmGST6T5BPd9iXdg76Pdw/+Pm/ZNa5JckGSe5L8YZInkrxjxc/tT3Y/B48muTPJt6zK+U1ye5Jnkzza2zfzXGbkF7uaH0ly6YrU+6HuZ+GRJP8xyQW9Yzd39R5L8n3LrrV37P1JKsnObnuh57apQJ/zgdXLdAp4f1XtAy4Hfryr7ybgvqraC9zXba+KG4EnetsfBH6+e+D3c4weAL4qfgH4rar6buBtjOpeyXOb5CLg7wD7q+otjN56+hpW5/zeARyY2He2c3klsLf7uB74yMtUY98dTNf728BbquqtwGeBmwG637lrgL/Q/ZsPd9nxcrmD6VpJcjHwvcAXersXe26rqpkP4B3Avb3tm4Gbl13Xi9T7n4D3AMeAC7t9FwLHll1bV8tuRr+4fw34BKNnYH8J2DHrfC+51tcAn6O7kN/bv6rndu05u69j9DbVnwC+b5XOL7AHeHSzcwn8CnDtrHHLrHfi2A8AH+tej+UCo2c5vGPZtQL3MJqIPAXs3Ipz29QMndkPrL5oSbW8qCR7gLcDnwbeUFVf7A79CfCGJZU16V8Cfx84022/HvhqVZ3qtlfp/F4CnAT+XbdE9G+SfCsrem6r6hngZxnNxr4IPA88xOqeXzj7uWzh9+5vA7/ZvV65epMcBJ6pqocnDi201tYCvQlJXgX8B+DvVtWf9o/V6M/w0ntFk3w/8GxVPbTsWua0A7gU+EhVvR34P0wsr6zKuQXo1p8PMvpD9O3AtzLjf8NX1Sqdy80k+QCj5c6PLbuWWZK8EvgHwC2bjX2pWgv0eR5YvVRJXsEozD9WVb/e7f5fSS7sjl8IPLus+nreCVyd5CngLkbLLr8AXNA96BtW6/yeAE5U1ae77XsYBfwqnluAdwOfq6qTVfUN4NcZnfNVPb9w9nO5sr93Sd4HfD/w3u6PEKxevd/J6A/7w93v227g95L8WRZca2uBPs8Dq5cmSRg9X/WJqvq53qH+Q7R/lNHa+lJV1c1Vtbuq9jA6j5+sqvcC9zN60DesSK0AVfUnwNNJvqvb9S7gcVbw3Ha+AFye5JXdz8VavSt5fjtnO5eHgR/pOjIuB57vLc0sTZIDjJYMr66qr/UOHQauSXJ+kksYXXD8H8uoEaCq/qCqvq2q9nS/byeAS7uf6cWe25f7wsYCLjZcxeiK9v8EPrDseiZq+yuM/jf1EeD3u4+rGK1N3wf8EfBfgdctu9aJuq8APtG9/g5GP/zHgY8D5y+7vl6d3wMc7c7vbwCvXeVzC/xj4A+BR4GPAuevyvkF7mS0tv+NLmCuO9u5ZHSx/Lbud+4PGHXurEK9xxmtP6/9rv1yb/wHunqPAVcuu9aJ40+xcVF0oefWW/8laZtobclFknQWBrokbRMGuiRtEwa6JG0TBrokbRMGuiRtEwa6JG0T/w+c4X/fRyuKPAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "prediction = estimator.predict(X_test)\n",
        "print(\"r_square score: \", r2_score(y_test,prediction))\n",
        "yy = np.array(y_test)\n",
        "predd = np.array(prediction)\n",
        "for i in range(10):\n",
        "  print(\"real value of y_test: \" + str(yy[i]) + \" -> the predict: \" + str(predd[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnQXNvh3-rVR",
        "outputId": "a89a1f43-45ac-4111-dd34-ab24222a81d9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47/47 [==============================] - 0s 1ms/step\n",
            "r_square score:  0.9998958133467363\n",
            "real value of y_test: 248.797897 -> the predict: 76.9234\n",
            "real value of y_test: 22.440866 -> the predict: 25.5887\n",
            "real value of y_test: 37.126244 -> the predict: 1.734948\n",
            "real value of y_test: 66.758057 -> the predict: 456.9687\n",
            "real value of y_test: 4.967855 -> the predict: 14.909012\n",
            "real value of y_test: 0.498976 -> the predict: 9.253459\n",
            "real value of y_test: 99999.0 -> the predict: 99999.0\n",
            "real value of y_test: 99999.0 -> the predict: 99998.96\n",
            "real value of y_test: 85.616592 -> the predict: 220.03487\n",
            "real value of y_test: 74.602707 -> the predict: 1022.2897\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "print(\"RMSE:\")\n",
        "sqrt(mean_squared_error(yy, predd)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSyzIFK5uzHP",
        "outputId": "080344d5-f934-4809-b560-42dc42aa6850"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "383.1512994106017"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}